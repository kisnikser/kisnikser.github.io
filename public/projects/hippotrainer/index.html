<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>HippoTrainer: Gradient-Based Hyperparameter Optimization for PyTorch | Nikita Kiselev</title>
<meta name="keywords" content="hyperparameter optimization, PyTorch, Optuna">
<meta name="description" content="We release a Python library for gradient-based hyperparameter optimization, implementing cutting-edge algorithms that leverage automatic differentiation to efficiently tune hyperparameters.">
<meta name="author" content="Daniil Dorin,&thinsp;Igor Ignashin,&thinsp;Nikita Kiselev,&thinsp;Andrey Veprikov">
<link rel="canonical" href="http://localhost:1313/projects/hippotrainer/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.177ebaef21802192b8610df8675c7cd36b682ac037b5d9bfad18dc908b59de1d.css" integrity="sha256-F3667yGAIZK4YQ34Z1x802toKsA3tdm/rRjckItZ3h0=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/projects/hippotrainer/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:title" content="HippoTrainer: Gradient-Based Hyperparameter Optimization for PyTorch" />
<meta property="og:description" content="We release a Python library for gradient-based hyperparameter optimization, implementing cutting-edge algorithms that leverage automatic differentiation to efficiently tune hyperparameters." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://localhost:1313/projects/hippotrainer/" /><meta property="article:section" content="projects" />
<meta property="article:published_time" content="2025-03-11T00:00:00+00:00" />
<meta property="article:modified_time" content="2025-03-11T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="HippoTrainer: Gradient-Based Hyperparameter Optimization for PyTorch"/>
<meta name="twitter:description" content="We release a Python library for gradient-based hyperparameter optimization, implementing cutting-edge algorithms that leverage automatic differentiation to efficiently tune hyperparameters."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Projects",
      "item": "http://localhost:1313/projects/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "HippoTrainer: Gradient-Based Hyperparameter Optimization for PyTorch",
      "item": "http://localhost:1313/projects/hippotrainer/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "HippoTrainer: Gradient-Based Hyperparameter Optimization for PyTorch",
  "name": "HippoTrainer: Gradient-Based Hyperparameter Optimization for PyTorch",
  "description": "We release a Python library for gradient-based hyperparameter optimization, implementing cutting-edge algorithms that leverage automatic differentiation to efficiently tune hyperparameters.",
  "keywords": [
    "hyperparameter optimization", "PyTorch", "Optuna"
  ],
  "articleBody": "In this blog-post we present our Python library HippoTrainer (or hippotrainer) for gradient-based hyperparameter optimization, implementing cutting-edge algorithms that leverage automatic differentiation to efficiently tune hyperparameters.\nIntroduction Hyperparameter tuning is time-consuming and computationally expensive, often requiring extensive trial and error to find optimal configurations. There is a variety of hyperparameter optimization methods, such as Grid Search, Random Search, Bayesian Optimization, etc. In the case of continuous hyperparameters, the gradient-based methods arise.\nWe implemented four effective and popular methods in one package, leveraging the unified, simple and clean structure. Below we delve into the problem statement and methods description.\nHyperparameter Optimization Problem Given a vector of model parameters $\\mathbf{w} \\in \\mathbb{R}^P$ and a vector of hyperparameters $\\boldsymbol{\\lambda} \\in \\mathbb{R}^H$. One aims to find optimal hyperparameters $\\boldsymbol{\\lambda}^*$, solving the bi-level optimization problem:\n$$ \\begin{aligned} \u0026\\boldsymbol{\\lambda}^* = \\argmin_{\\boldsymbol{\\lambda}} \\mathcal{L}_{\\text{val}}(\\mathbf{w}^*, \\boldsymbol{\\lambda}), \\\\ \\text{s.t. } \u0026\\mathbf{w}^* = \\argmin_{\\mathbf{w}} \\mathcal{L}_{\\text{train}}(\\mathbf{w}, \\boldsymbol{\\lambda}) \\end{aligned} $$\nOften $\\mathbf{w}$ are optimized with gradient descent, so unrolled optimization is typically used:\n$$ \\mathbf{w}_{t+1} = \\boldsymbol{\\Phi}(\\mathbf{w}_{t}, \\boldsymbol{\\lambda}), \\quad t = 0, \\ldots, T-1. $$\nTypical way to optimize continuous hyperparameters is the gradient-based optimization that involves automatic differentiation through this unrolled optimization formula.\nHypergradient Calculation Chain rule gives us a hypergradient $d_{\\boldsymbol{\\lambda}} \\mathcal{L}_{\\text{val}}(\\mathbf{w}_T, \\boldsymbol{\\lambda})$, viewing $\\mathbf{w}_T$ as a function of $\\boldsymbol{\\lambda}$: $$ \\underbrace{d_{\\boldsymbol{\\lambda}} \\mathcal{L}_{\\text{val}}(\\mathbf{w}_T, \\boldsymbol{\\lambda})}_{\\text{hypergradient}} = \\underbrace{\\nabla_{\\boldsymbol{\\lambda}} \\mathcal{L}_{\\text{val}}(\\mathbf{w}_T, \\boldsymbol{\\lambda})}_{\\text{hyperparam direct grad.}} + \\underbrace{\\nabla_{\\mathbf{w}} \\mathcal{L}_{\\text{val}}(\\mathbf{w}_T, \\boldsymbol{\\lambda})}_{\\text{parameter direct grad.}} \\times \\underbrace{\\frac{d\\mathbf{w}_T}{d\\boldsymbol{\\lambda}}}_{\\text{\\textbf{best-response Jacobian}}} $$\nHere best-response Jacobian is hard to compute! Typical Solution — Implicit Function Theorem: $$ \\frac{d\\mathbf{w}_T}{d\\boldsymbol{\\lambda}} = - \\underbrace{\\left[ \\nabla^2_{\\mathbf{w}} \\mathcal{L}_{\\text{train}}(\\mathbf{w}_T, \\boldsymbol{\\lambda}) \\right]^{-1}}_{\\text{\\textbf{inverted} training Hessian}} \\times \\underbrace{\\nabla_{\\mathbf{w}} \\nabla_{\\boldsymbol{\\lambda}} \\mathcal{L}_{\\text{train}} (\\mathbf{w}_T, \\boldsymbol{\\lambda})}_{\\text{training mixed partials}}. $$\nHessian inversion is a cornerstone of many algorithms. The next section contains information about each of the methods presented in our library, as they can be generalized to solve the above problem in different ways.\nMethods To exactly invert a $P \\times P$ Hessian, we require $\\mathcal{O}(P^3)$ operations, which is intractable for modern NNs. We can efficiently approximate the inverse with the Neumann series: $$ \\left[ \\nabla^2_{\\mathbf{w}} \\mathcal{L}_{\\text{train}}(\\mathbf{w}_T, \\boldsymbol{\\lambda}) \\right]^{-1} = \\lim_{i \\to \\infty} \\sum_{j=0}^{i} \\left[ \\mathbf{I} - \\nabla^2_{\\mathbf{w}} \\mathcal{L}_{\\text{train}} (\\mathbf{w}_T, \\boldsymbol{\\lambda}) \\right]^j. $$\nUsing different numbers of terms in this series, one can derive a list of methods.\nT1 – T2 (Luketina et al. 2015) In this method, the number of terms $i$ equals $0$, and the number of inner optimization steps $T$ is equal to $1$. Therefore, this method is also named Greedy gradient-based hyperparameter optimization. In particular, here we have: $$ \\left[ \\nabla^2_{\\mathbf{w}} \\mathcal{L}_{\\text{train}}(\\mathbf{w}_T, \\boldsymbol{\\lambda}) \\right]^{-1} \\approx \\mathbf{I}. $$\nIFT (Lorraine et al. 2019) Another method uses a pre-determined number of terms in the Neumann series. It also efficiently computes $\\nabla_{\\boldsymbol{\\lambda}} \\mathcal{L}_{\\text{val}}(\\mathbf{w}_T, \\boldsymbol{\\lambda}) \\times \\left[ \\nabla^2_{\\mathbf{w}} \\mathcal{L}_{\\text{train}}(\\mathbf{w}_T, \\boldsymbol{\\lambda}) \\right]^{-1}$, leveraging the following approximation formula: $$ \\left[ \\nabla^2_{\\mathbf{w}} \\mathcal{L}_{\\text{train}}(\\mathbf{w}_T, \\boldsymbol{\\lambda}) \\right]^{-1} \\approx \\sum_{j=0}^{i} \\left[ \\mathbf{I} - \\nabla^2_{\\mathbf{w}} \\mathcal{L}_{\\text{train}} (\\mathbf{w}_T, \\boldsymbol{\\lambda}) \\right]^j. $$\nHOAG (Pedregosa, 2016) In contrast to the previous ones, this method solves the linear system using the Conjugate Gradient to invert the Hessian approximately. The following system is solved w.r.t. $\\mathbf{z}$: $$ \\nabla^2_{\\mathbf{w}} \\mathcal{L}_{\\text{train}}(\\mathbf{w}_T, \\boldsymbol{\\lambda}) \\cdot \\mathbf{z} = \\nabla_{\\boldsymbol{\\lambda}} \\mathcal{L}_{\\text{val}}(\\mathbf{w}_T, \\boldsymbol{\\lambda}). $$\nDrMAD (Fu et al. 2016) The last method in our package is not straightforward. Instead of storing all intermediate weights $\\mathbf{w}_0, \\ldots, \\mathbf{w}_T$, it approximates the training trajectory as a linear combination of the initial $\\mathbf{w}_0$ and final $\\mathbf{w}_T$ weights: $$ \\mathbf{w}(\\beta) = (1 - \\beta) \\mathbf{w}_0 + \\beta \\mathbf{w}_T, \\quad 0 \u003c \\beta \u003c 1. $$ Then it uses such approximation to perform the backward pass on the hyperparameters.\nImplementation (see our GitHub for details) TODO\nDemo TODO\nConclusion TODO\nReferences [1] Luketina et al. “Scalable Gradient-Based Tuning of Continuous Regularization Hyperparameters”. arXiv preprint arXiv:1511.06727 (2015).\n[2] Lorraine et al. “Optimizing Millions of Hyperparameters by Implicit Differentiation”. arXiv preprint arXiv:1911.02590 (2019).\n[3] Pedregosa. “Hyperparameter optimization with approximate gradient”. arXiv preprint arXiv:1602.02355 (2016).\n[4] Fu et al. “DrMAD: Distilling Reverse-Mode Automatic Differentiation for Optimizing Hyperparameters of Deep Neural Networks”. arXiv preprint arXiv:1601.00917 (2016).\n",
  "wordCount" : "651",
  "inLanguage": "en",
  "datePublished": "2025-03-11T00:00:00Z",
  "dateModified": "2025-03-11T00:00:00Z",
  "author":[{
    "@type": "Person",
    "name": "Daniil Dorin"
  }, {
    "@type": "Person",
    "name": "Igor Ignashin"
  }, {
    "@type": "Person",
    "name": "Nikita Kiselev"
  }, {
    "@type": "Person",
    "name": "Andrey Veprikov"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/projects/hippotrainer/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Nikita Kiselev",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js" integrity="sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"
  onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          delimiters: [
            {left: '$$', right: '$$', display: true},
            {left: '$', right: '$', display: false},
            {left: "\\begin{equation}", right: "\\end{equation}", display: true},
            {left: "\\begin{equation*}", right: "\\end{equation*}", display: true},
            {left: "\\begin{align}", right: "\\end{align}", display: true},
            {left: "\\begin{align*}", right: "\\end{align*}", display: true},
            {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
            {left: "\\begin{gather}", right: "\\end{gather}", display: true},
            {left: "\\begin{CD}", right: "\\end{CD}", display: true},
          ],
          throwOnError : false
        });
    });
</script>
 


</head>

<body class="" id="top">

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Nikita Kiselev">
                <img src="http://localhost:1313/favicon.ico" alt="" aria-label="logo"
                    height="18"
                    width="18">Nikita Kiselev</a>
            <div class="logo-switches">
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/publications/" title="Publications">
                    <span>Publications</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/talks/" title="Talks">
                    <span>Talks</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/projects/" title="Projects">
                    <span>Projects</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/resume.pdf" title="Resume">
                    <span>Resume</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/cv.pdf" title="CV">
                    <span>CV</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/archive/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
        </ul>
    </nav>
</header>

    <main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      HippoTrainer: Gradient-Based Hyperparameter Optimization for PyTorch
    </h1>
    <div class="post-meta"><span title='2025-03-11 00:00:00 +0000 UTC'>March 2025</span>&nbsp;&middot;&nbsp;4 min&nbsp;&middot;&nbsp;Daniil Dorin,&thinsp;Igor Ignashin,&thinsp;Nikita Kiselev,&thinsp;Andrey Veprikov&nbsp;&middot;&nbsp;<a href="https://github.com/intsystems/hippotrainer" rel="noopener noreferrer" target="_blank">GitHub</a>

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><nav id="TableOfContents">
  <ul>
    <li><a href="#introduction-a-nameintroductiona">Introduction <a name="introduction"></a></a></li>
    <li><a href="#methods">Methods</a></li>
    <li><a href="#implementation-see-our-githubhttpsgithubcomintsystemshippotrainer-for-details">Implementation (see our <a href="https://github.com/intsystems/hippotrainer">GitHub</a> for details)</a></li>
    <li><a href="#demo">Demo</a></li>
    <li><a href="#conclusion">Conclusion</a></li>
    <li><a href="#references">References</a></li>
  </ul>
</nav>
        </div>
    </details>
</div>

  <div class="post-content"><p>In this blog-post we present our Python library <a href="https://github.com/intsystems/hippotrainer" target="_blank">HippoTrainer</a> (or <code>hippotrainer</code>) for gradient-based hyperparameter optimization, implementing cutting-edge algorithms that leverage automatic differentiation to efficiently tune hyperparameters.</p>
<h2 id="introduction-a-nameintroductiona">Introduction <a name="introduction"></a></h2>
<p>Hyperparameter tuning is time-consuming and computationally expensive, often requiring extensive trial and error to find optimal configurations. There is a variety of hyperparameter optimization methods, such as Grid Search, Random Search, Bayesian Optimization, etc. In the case of continuous hyperparameters, the gradient-based methods arise.</p>
<p>We implemented four effective and popular methods in one package, leveraging the unified, simple and clean structure. Below we delve into the problem statement and methods description.</p>
<h3 id="hyperparameter-optimization-problem">Hyperparameter Optimization Problem</h3>
<p>Given a vector of model parameters $\mathbf{w} \in \mathbb{R}^P$ and a vector of hyperparameters $\boldsymbol{\lambda} \in \mathbb{R}^H$. One aims to find optimal hyperparameters $\boldsymbol{\lambda}^*$, solving the bi-level optimization problem:</p>
<p>$$
\begin{aligned}
&amp;\boldsymbol{\lambda}^* = \argmin_{\boldsymbol{\lambda}} \mathcal{L}_{\text{val}}(\mathbf{w}^*, \boldsymbol{\lambda}), \\
\text{s.t. } &amp;\mathbf{w}^* = \argmin_{\mathbf{w}} \mathcal{L}_{\text{train}}(\mathbf{w}, \boldsymbol{\lambda})
\end{aligned}
$$</p>
<p>Often $\mathbf{w}$ are optimized with gradient descent, so <strong>unrolled optimization</strong> is typically used:</p>
<p>$$
\mathbf{w}_{t+1} = \boldsymbol{\Phi}(\mathbf{w}_{t}, \boldsymbol{\lambda}), \quad t = 0, \ldots, T-1.
$$</p>
<p>Typical way to optimize continuous hyperparameters is the <strong>gradient-based optimization</strong> that involves automatic differentiation through this unrolled optimization formula.</p>
<h3 id="hypergradient-calculation">Hypergradient Calculation</h3>
<p>Chain rule gives us a hypergradient $d_{\boldsymbol{\lambda}} \mathcal{L}_{\text{val}}(\mathbf{w}_T, \boldsymbol{\lambda})$, viewing $\mathbf{w}_T$ as a function of $\boldsymbol{\lambda}$:
$$
\underbrace{d_{\boldsymbol{\lambda}} \mathcal{L}_{\text{val}}(\mathbf{w}_T, \boldsymbol{\lambda})}_{\text{hypergradient}} = \underbrace{\nabla_{\boldsymbol{\lambda}} \mathcal{L}_{\text{val}}(\mathbf{w}_T, \boldsymbol{\lambda})}_{\text{hyperparam direct grad.}} + \underbrace{\nabla_{\mathbf{w}} \mathcal{L}_{\text{val}}(\mathbf{w}_T, \boldsymbol{\lambda})}_{\text{parameter direct grad.}} \times \underbrace{\frac{d\mathbf{w}_T}{d\boldsymbol{\lambda}}}_{\text{\textbf{best-response Jacobian}}}
$$</p>
<ul>
<li>Here <strong>best-response Jacobian</strong> is hard to compute!</li>
</ul>
<p>Typical Solution — Implicit Function Theorem:
$$
\frac{d\mathbf{w}_T}{d\boldsymbol{\lambda}} = - \underbrace{\left[ \nabla^2_{\mathbf{w}} \mathcal{L}_{\text{train}}(\mathbf{w}_T, \boldsymbol{\lambda}) \right]^{-1}}_{\text{\textbf{inverted} training Hessian}} \times \underbrace{\nabla_{\mathbf{w}} \nabla_{\boldsymbol{\lambda}} \mathcal{L}_{\text{train}} (\mathbf{w}_T, \boldsymbol{\lambda})}_{\text{training mixed partials}}.
$$</p>
<ul>
<li>Hessian <strong>inversion</strong> is a cornerstone of many algorithms.</li>
</ul>
<p>The next section contains information about each of the methods presented in our library, as they can be generalized to solve the above problem in different ways.</p>
<h2 id="methods">Methods</h2>
<p>To exactly invert a $P \times P$ Hessian, we require $\mathcal{O}(P^3)$ operations, which is intractable for modern NNs. We can efficiently approximate the inverse with the Neumann series:
$$
\left[ \nabla^2_{\mathbf{w}} \mathcal{L}_{\text{train}}(\mathbf{w}_T, \boldsymbol{\lambda}) \right]^{-1} = \lim_{i \to \infty} \sum_{j=0}^{i} \left[ \mathbf{I} - \nabla^2_{\mathbf{w}} \mathcal{L}_{\text{train}} (\mathbf{w}_T, \boldsymbol{\lambda}) \right]^j.
$$</p>
<p>Using different numbers of terms in this series, one can derive a list of methods.</p>
<h3 id="t1--t2-luketina-et-al-2015httpsarxivorgabs151106727">T1 – T2 (<a href="https://arxiv.org/abs/1511.06727" target="_blank">Luketina et al. 2015</a>)</h3>
<p>In this method, the number of terms $i$ equals $0$, and the number of inner optimization steps $T$ is equal to $1$. Therefore, this method is also named Greedy gradient-based hyperparameter optimization. In particular, here we have:
$$
\left[ \nabla^2_{\mathbf{w}} \mathcal{L}_{\text{train}}(\mathbf{w}_T, \boldsymbol{\lambda}) \right]^{-1} \approx \mathbf{I}.
$$</p>
<h3 id="ift-lorraine-et-al-2019httpsarxivorgabs191102590">IFT (<a href="https://arxiv.org/abs/1911.02590" target="_blank">Lorraine et al. 2019</a>)</h3>
<p>Another method uses a pre-determined number of terms in the Neumann series. It also efficiently computes $\nabla_{\boldsymbol{\lambda}} \mathcal{L}_{\text{val}}(\mathbf{w}_T, \boldsymbol{\lambda}) \times \left[ \nabla^2_{\mathbf{w}} \mathcal{L}_{\text{train}}(\mathbf{w}_T, \boldsymbol{\lambda}) \right]^{-1}$, leveraging the following approximation formula:
$$
\left[ \nabla^2_{\mathbf{w}} \mathcal{L}_{\text{train}}(\mathbf{w}_T, \boldsymbol{\lambda}) \right]^{-1} \approx \sum_{j=0}^{i} \left[ \mathbf{I} - \nabla^2_{\mathbf{w}} \mathcal{L}_{\text{train}} (\mathbf{w}_T, \boldsymbol{\lambda}) \right]^j.
$$</p>
<h3 id="hoag-pedregosa-2016httpsarxivorgabs160202355">HOAG (<a href="https://arxiv.org/abs/1602.02355" target="_blank">Pedregosa, 2016</a>)</h3>
<p>In contrast to the previous ones, this method solves the linear system using the Conjugate Gradient to invert the Hessian approximately. The following system is solved w.r.t. $\mathbf{z}$:
$$
\nabla^2_{\mathbf{w}} \mathcal{L}_{\text{train}}(\mathbf{w}_T, \boldsymbol{\lambda}) \cdot \mathbf{z} = \nabla_{\boldsymbol{\lambda}} \mathcal{L}_{\text{val}}(\mathbf{w}_T, \boldsymbol{\lambda}).
$$</p>
<h3 id="drmad-fu-et-al-2016httpsarxivorgabs160100917">DrMAD (<a href="https://arxiv.org/abs/1601.00917" target="_blank">Fu et al. 2016</a>)</h3>
<p>The last method in our package is not straightforward. Instead of storing all intermediate weights $\mathbf{w}_0, \ldots, \mathbf{w}_T$, it approximates the training trajectory as a linear combination of the initial $\mathbf{w}_0$ and final $\mathbf{w}_T$ weights:
$$
\mathbf{w}(\beta) = (1 - \beta) \mathbf{w}_0 + \beta \mathbf{w}_T, \quad 0 &lt; \beta &lt; 1.
$$
Then it uses such approximation to perform the backward pass on the hyperparameters.</p>
<h2 id="implementation-see-our-githubhttpsgithubcomintsystemshippotrainer-for-details">Implementation (see our <a href="https://github.com/intsystems/hippotrainer" target="_blank">GitHub</a> for details)</h2>
<p>TODO</p>
<h2 id="demo">Demo</h2>
<p>TODO</p>
<h2 id="conclusion">Conclusion</h2>
<p>TODO</p>
<h2 id="references">References</h2>
<p>[1] Luketina et al. <a href="https://arxiv.org/abs/1511.06727" target="_blank">&ldquo;Scalable Gradient-Based Tuning of Continuous Regularization Hyperparameters&rdquo;</a>. arXiv preprint arXiv:1511.06727 (2015).</p>
<p>[2] Lorraine et al. <a href="https://arxiv.org/abs/1911.02590" target="_blank">&ldquo;Optimizing Millions of Hyperparameters by Implicit Differentiation&rdquo;</a>. arXiv preprint arXiv:1911.02590 (2019).</p>
<p>[3] Pedregosa. <a href="https://arxiv.org/abs/1602.02355" target="_blank">&ldquo;Hyperparameter optimization with approximate gradient&rdquo;</a>. arXiv preprint arXiv:1602.02355 (2016).</p>
<p>[4] Fu et al. <a href="https://arxiv.org/abs/1601.00917" target="_blank">&ldquo;DrMAD: Distilling Reverse-Mode Automatic Differentiation for Optimizing Hyperparameters of Deep Neural Networks&rdquo;</a>. arXiv preprint arXiv:1601.00917 (2016).</p>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/hyperparameter-optimization/">Hyperparameter Optimization</a></li>
      <li><a href="http://localhost:1313/tags/pytorch/">PyTorch</a></li>
      <li><a href="http://localhost:1313/tags/optuna/">Optuna</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="http://localhost:1313/">Nikita Kiselev</a></span> ·     
    <span>
    Powered by 
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/pmichaillat/hugo-website/" rel="noopener" target="_blank">a modified version</a>
         of 
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>
</html>
