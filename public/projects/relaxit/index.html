<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Just Relax It! Leveraging relaxation for discrete variables optimization | Nikita Kiselev</title>
<meta name="keywords" content="Relaxation, Gumbel-Softmax, Straight-Through Estimator, Python, Library, Package, PyTorch, Pyro">
<meta name="description" content="We release a cutting-edge Python library designed to streamline the optimization of discrete probability distributions in neural networks, offering a suite of advanced relaxation techniques compatible with PyTorch.">
<meta name="author" content="Daniil Dorin,&thinsp;Igor Ignashin,&thinsp;Nikita Kiselev,&thinsp;Andrey Veprikov">
<link rel="canonical" href="http://localhost:1313/projects/relaxit/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.4582bb023ced504ba996f5488ce21d28596094f4d74f23b448645b0853334c4b.css" integrity="sha256-RYK7AjztUEuplvVIjOIdKFlglPTXTyO0SGRbCFMzTEs=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/projects/relaxit/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:title" content="Just Relax It! Leveraging relaxation for discrete variables optimization" />
<meta property="og:description" content="We release a cutting-edge Python library designed to streamline the optimization of discrete probability distributions in neural networks, offering a suite of advanced relaxation techniques compatible with PyTorch." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://localhost:1313/projects/relaxit/" />
<meta property="og:image" content="http://localhost:1313/overview.png" /><meta property="article:section" content="projects" />
<meta property="article:published_time" content="2024-12-07T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-12-07T00:00:00+00:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="http://localhost:1313/overview.png" />
<meta name="twitter:title" content="Just Relax It! Leveraging relaxation for discrete variables optimization"/>
<meta name="twitter:description" content="We release a cutting-edge Python library designed to streamline the optimization of discrete probability distributions in neural networks, offering a suite of advanced relaxation techniques compatible with PyTorch."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Projects",
      "item": "http://localhost:1313/projects/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Just Relax It! Leveraging relaxation for discrete variables optimization",
      "item": "http://localhost:1313/projects/relaxit/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Just Relax It! Leveraging relaxation for discrete variables optimization",
  "name": "Just Relax It! Leveraging relaxation for discrete variables optimization",
  "description": "We release a cutting-edge Python library designed to streamline the optimization of discrete probability distributions in neural networks, offering a suite of advanced relaxation techniques compatible with PyTorch.",
  "keywords": [
    "Relaxation", "Gumbel-Softmax", "Straight-Through Estimator", "Python", "Library", "Package", "PyTorch", "Pyro"
  ],
  "articleBody": " In this blog-post we present our Python library “Just Relax It” (or relaxit) designed to streamline the optimization of discrete probability distributions in neural networks, offering a suite of advanced relaxation techniques compatible with PyTorch.\nIntroduction Recent development of generative models, e.g. VAE and Diffusion Models, has driven relevant mathematical tools. Any generative model contains some source of randomness to make new objects. This randomness represents a certain probability distribution, from which random variables are sampled. Thus, the task of training a generative model is often comes down to optimization of such distribution parameters.\nPioneering generative models work with continous distributions like Normal one. However, for some modalities, e.g. texts or graphs, it is quite natural to use discrete distributions – Bernoulli, Categorical, etc.\nThus, we present our new Python library “Just Relax It” that combines the best techniques for relaxing discrete distributions (we will explain what that means later) into an easy-to-use package. And it is compatible with PyTorch!\nWe start with a basic example that shows how parameter optimization typically happens for continuous distributions, then we move on smoothly to the case of discrete distributions. After that, we talk about the main relaxation techniques used in our library and make a demo of training a VAE with discrete latent variables.\nVAE example Fig. 1. Variational Autoencoder (VAE) architecture. The original VAE (Kingma \u0026 Welling, 2014) consists of two parts:\nEncoder $q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x})$, which is represented by a neural network $g_{\\boldsymbol{\\phi}}(\\mathbf{x})$ that outputs parameters of the latent Gaussian distribution; Decoder $p_{\\boldsymbol{\\theta}}(\\mathbf{x}|\\mathbf{z})$, which is represented by a neural network $f_{\\boldsymbol{\\theta}}(\\mathbf{z})$ that outputs parameters of the sample distribution (typically Gaussian or Bernoulli). The math behind training a VAE is not obvious actually, so we will just focus on the ELBO (evidence lower bound), which needs to be maximized w.r.t. the parameters of the encoder and decoder: $$ \\mathcal{L}{\\boldsymbol{\\phi}, \\boldsymbol{\\theta}}(\\mathbf{x}) = \\mathbb{E}{q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x})} \\log p_{\\boldsymbol{\\theta}}(\\mathbf{x}|\\mathbf{z}) - KL(q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x}) | p(\\mathbf{z})) \\to \\max_{\\boldsymbol{\\phi}, \\boldsymbol{\\theta}}. $$\nDuring the M-step, we gonna derive the unbiased estimator for the gradient $\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}_{\\boldsymbol{\\phi}, \\boldsymbol{\\theta}}(\\mathbf{x})$:\n$$ \\begin{aligned} \\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}{\\boldsymbol{\\phi}, \\boldsymbol{\\theta}}(\\mathbf{x}) \u0026= \\textcolor{blue}{\\nabla{\\boldsymbol{\\theta}}} \\int q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x}) \\log p_{\\boldsymbol{\\theta}}(\\mathbf{x}|\\mathbf{z}) d\\mathbf{z} \\ \u0026= \\int q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x}) \\textcolor{blue}{\\nabla_{\\boldsymbol{\\theta}}} \\log p_{\\boldsymbol{\\theta}}(\\mathbf{x}|\\mathbf{z}) d\\mathbf{z} \\ \u0026\\approx \\nabla_{\\boldsymbol{\\theta}} \\log p_{\\boldsymbol{\\theta}}(\\mathbf{x}|\\mathbf{z}^), \\quad \\mathbf{z}^ \\sim q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x}), \\end{aligned} $$\nwhere the last approximation is a Monte-Carlo sampling estimator.\nHowever, on the E-step it is quite tricky to get unbiased estimator for the gradient $$\\nabla_{\\boldsymbol{\\phi}}\\mathcal{L}_{\\boldsymbol{\\phi}, \\boldsymbol{\\theta}}(\\mathbf{x}).$$\nAs density function $q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x})$ depends on the parameters $\\boldsymbol{\\phi}$, it is impossible to use the Monte-Carlo estimation:\n$$ \\begin{aligned} \\nabla_{\\boldsymbol{\\phi}}\\mathcal{L}{\\boldsymbol{\\phi}, \\boldsymbol{\\theta}}(\\mathbf{x}) \u0026= \\textcolor{blue}{\\nabla{\\boldsymbol{\\phi}}} \\int q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x}) \\log p_{\\boldsymbol{\\theta}}(\\mathbf{x}|\\mathbf{z}) d\\mathbf{z} - \\nabla_{\\boldsymbol{\\phi}} KL(q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x}) | p(\\mathbf{z})) \\ \u0026\\textcolor{red}{\\neq} \\int q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x}) \\textcolor{blue}{\\nabla_{\\boldsymbol{\\theta}}} \\log p_{\\boldsymbol{\\theta}}(\\mathbf{x}|\\mathbf{z}) d\\mathbf{z} - \\nabla_{\\boldsymbol{\\phi}} KL(q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x}) | p(\\mathbf{z})), \\ \\end{aligned} $$\nand this is the moment where the reparameterization trick arises, we reparameterize the outputs of the encoder:\n$$ \\begin{aligned} \\nabla_{\\boldsymbol{\\phi}} \\int \\textcolor{blue}{q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x})} \\log p_{\\boldsymbol{\\theta}}(\\mathbf{x}|\\textcolor{OliveGreen}{\\mathbf{z}}) d\\mathbf{z} \u0026= \\int \\textcolor{blue}{p(\\boldsymbol{\\epsilon})} \\nabla_{\\boldsymbol{\\phi}} \\log p_{\\boldsymbol{\\theta}}(\\mathbf{x}|\\textcolor{OliveGreen}{\\mathbf{g}{\\boldsymbol{\\phi}}(\\mathbf{x}, \\boldsymbol{\\epsilon})}) d\\boldsymbol{\\epsilon} \\ \u0026\\approx \\nabla{\\boldsymbol{\\phi}} \\log p_{\\boldsymbol{\\theta}}(\\mathbf{x}|\\textcolor{OliveGreen}{\\boldsymbol{\\sigma}{\\boldsymbol{\\phi}}(\\mathbf{x})} \\odot \\textcolor{blue}{\\boldsymbol{\\epsilon}^*} + \\textcolor{OliveGreen}{\\boldsymbol{\\mu}{\\boldsymbol{\\phi}}(\\mathbf{x})}), \\quad \\textcolor{blue}{\\boldsymbol{\\epsilon}^*} \\sim \\mathcal{N}(0, \\mathbf{I}), \\end{aligned} $$\nso we move the randomness to the $\\boldsymbol{\\epsilon} \\sim p(\\boldsymbol{\\epsilon})$, and use the deterministic transform $$\\mathbf{z} = \\mathbf{g}{\\boldsymbol{\\phi}}(\\mathbf{x}, \\boldsymbol{\\epsilon})$$ in order to get unbiased gradient. It also needs to be mentioned that normal assumptions for $q{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x})$ and $p(\\mathbf{z})$ allows us to compute $KL$ analytically and thus calculate the gradient $\\nabla_{\\boldsymbol{\\phi}}KL(q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x}) | p(\\mathbf{z}))$.\nThe above example gives us an understanding of a crucial reparameterization trick, which allows us to get unbiased gradient estimations for the continuous latent space in VAE model. But actually discrete representations $\\mathbf{z}$ are potentially a more natural fit for many of the modalities (like texts or images), which moves us to the discrete VAE latentes. Therefore\nOur encoder should output discrete distribution; We need the analogue of the reparameterization trick for the discrete distribution; Our decoder should input discrete random variable. The classical solution for the discrete variables reparameterization trick is Gumbel-Softmax (Jang et al. 2017) or Concrete relaxation (Maddison et al. 2017).\nGumbel distribution $$ g \\sim \\mathrm{Gumbel}(0, 1) \\quad \\Leftrightarrow \\quad g = -\\log( - \\log u), \\quad u \\sim \\mathrm{Uniform}[0, 1] $$\nTheorem (Gumbel-Max trick) Let $g_k \\sim \\mathrm{Gumbel}(0, 1)$ for $k = 1, \\ldots, K$. Then a discrete random variable\n$$ c = \\arg\\max_k [\\log \\pi_k + g_k] $$\nhas a categorical distribution $c \\sim \\mathrm{Categorical}(\\boldsymbol{\\pi})$.\nWe could sample from the discrete distribution using Gumbel-Max reparameterization; Here parameters and random variable sampling are separated (reparameterization trick); Problem: we still have non-differentiable $\\arg\\max$ operation. Gumbel-Softmax relaxation $$ \\hat{\\mathbf{c}} = \\mathrm{softmax}\\left( \\frac{\\log q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x}) + \\mathbf{g}}{\\tau} \\right) $$\nHere $\\tau$ is a temperature parameters. Now we have differentiable operation, but the gradient estimator is biased now. However, if $\\tau \\to 0$, then the estimation becomes more and more accurate.\nOther relaxation methods So far, we have talked about one possible example of a discrete variable relaxation (VAE with discrete latent variables) and the classical approach to solving this problem. However, the Gumbel-Softmax relaxation was actually proposed a long time ago. There are actually many other relaxation techniques that can provide more flexible and accurate (and even unbiased) gradient estimates. The rest of our blog-post will focus on cutting-edge relaxation techniques and how we built a Python library that uses them, which works with the PyTorch framework to train neural networks efficiently.\nAlgorithms In this section, we provide a short description for each of the implemented methods. We can generalize them as follows. Suppose that $x$ is a random variable, $f$ if a function (say, the loss function), and we are interested in computing $\\frac{\\partial}{\\partial \\theta} \\mathbb{E}_{x}\\left[ f(x) \\right]$. It is quite natural decision because typical ML problem looks like this. So, two different ideas exist:\nScore function (SF) estimator. In this case, we are given a parameterized probability distribution $x \\sim p(\\cdot; \\theta)$ and use $$ \\frac{\\partial}{\\partial \\theta} \\mathbb{E}{x}\\left[ f(x) \\right] = \\mathbb{E}{x} \\left[ f(x) \\frac{\\partial}{\\partial \\theta} \\log p(x; \\theta) \\right]. $$\nPathwise derivative (PD) estimator. In this case $x$ is a determinisitc, differentiable function of $\\theta$ and another random variable $z$, i.e. we can write $x(z, \\theta)$: $$ \\frac{\\partial}{\\partial \\theta} \\mathbb{E}{x}\\left[ f(x(z, \\theta)) \\right] = \\mathbb{E}{z} \\left[ \\frac{\\partial}{\\partial \\theta} f(x(z, \\theta)) \\right]. $$\nThe latter one we have seen previously in the VAE example! A sample $x$ from $\\mathcal{N}(\\mu, \\sigma^2)$ can be obtained by sampling $z$ from the standard normal distribution $\\mathcal{N}(0, 1)$ and then transforming it using $x(z, \\theta) = \\sigma z + \\mu$. And this is called reparameterization trick.\nHowever, when $x$ is a discrete variable, it is quite tricky to make a pathwise derivative estimator, i.e. to reparameterize the discrete distribution. And this is the moment of relaxation! We replace $x$ with a continuous relaxation $x(z, \\theta) \\approx x_{\\tau}(z, \\theta)$, where $\\tau \u003e 0$ is a temperature that controls the tightness of the relaxaton (at low temperatues, the relaxation is nearly high).\nRelaxed Bernoulli (Yamada, Lindenbaum et al. 2018) The reparameterization trick is inspired by the idea of stochastic gates and aims to approximate a Bernoulli random variable in a more relaxed manner. This technique involves drawing a random variable, denoted as $\\epsilon$, from a normal distribution with a mean of 0 and a variance of $\\sigma^2$, where $\\sigma$ is a fixed parameter. The random variable $\\epsilon$ is then used to compute $z$ as follows:\n$$ \\begin{aligned} \\epsilon \u0026\\sim \\mathcal{N}(0, \\sigma^2),\\ z \u0026= \\min (1, \\max (0, \\mu + \\epsilon)), \\end{aligned} $$\nwhere $\\mu$ is a learnable parameter that can be tuned during the training process. This transformation ensures that the resulting $z$ value is bounded between 0 and 1, thereby relaxing the Bernoulli distribution.\nCorrelated relaxed Bernoulli (Lee, Imrie et al. 2022) This method generates correlated gate vectors from a multivariate Bernoulli distribution using a Gaussian copula:\n$$C_R(U_1, \\ldots, U_p) = \\Phi_R(\\Phi^{-1}(U_1), \\ldots, \\Phi^{-1}(U_p)),$$\nwhere $\\Phi_R$ is the joint CDF of a multivariate Gaussian distribution with correlation matrix $R$, and $\\Phi^{-1}$ is the inverse CDF of the standard univariate Gaussian distribution.\nThe gate vector $m$ is generated as:\n$$m_k = \\begin{cases} 1, \u0026 \\text{if } U_k \\leq \\pi_k, \\ 0, \u0026 \\text{if } U_k \u003e \\pi_k, \\end{cases}\\quad k = 1, \\ldots, p, $$\nwhere $U_k$ are correlated random variables preserving the input feature correlations.\nFor differentiability, a continuous relaxation is applied:\n$$m_k = \\sigma \\left( \\frac{1}{\\tau} \\left( \\log \\frac{U_k}{1 - U_k} + \\log \\frac{\\pi_k}{1 - \\pi_k} \\right) \\right),$$\nwhere $\\sigma(x) = \\frac{1}{1 + \\exp(-x)}$ is the sigmoid function, and $\\tau$ is a temperature hyperparameter. Thus, the Bernolli distribution relaxes.\nGumbel-Softmax TOP-K (Kool et al. 2019) Suppose we want to get $K$ samples without replacement (i.e., not repeating) according to the Categorical distribution with probabilities $\\boldsymbol{\\pi}$. Similar to the Gumbel-Max method, let $g_k \\sim \\mathrm{Gumbel}(0, 1)$ for $k = 1, \\ldots, K$, then the Gumbel-Max-Top$K$ Theorem says, that the values of the form\n$$c_1, \\ldots , c_K = \\text{Arg}\\underset{k}{\\text{top}}K [ \\log\\pi_k + g_k]$$\nhave the $\\mathrm{Categorical}(\\boldsymbol{\\pi})$ distribution without replacement.\nThis approach has all the same pros and cons as the classical Gumbel-Max trick, however, they can be fixed with the Gumbel-Softmax relaxation using a simple loop:\n$$ Algorithm $$\nTherefore, this method allows us to relax the Categorical distribution.\nStochastic Times Smooth (Bengio et al. 2013) The Stochastic Times Smooth distribution can be written as follows\n$$ \\begin{aligned} p_i \u0026= \\sigma(a_i), \\ b_i \u0026\\sim \\text{Binomial}(\\sqrt{p_i}), \\ h_i \u0026= b_i \\sqrt{p_i}, \\end{aligned} $$\nwhere $a_i$ is a parameter of this distribution. This one provides a Bernoulli distribution relaxation.\nInvertible Gaussian (Potapczynski et al. 2019) The idea of this method is to remove interpretability of parameters in Gumbel-Softmax relaxation, and achieve then higher quality. Namely, the goal of Gumbel-Softmax relaxation is to relax $\\mathbf{z} \\sim \\mathrm{Cat}(\\boldsymbol{\\pi})$ proposing temperature parameter $\\tau \\to 0$, which concentrates mass on vertices:\n$$\\tilde{\\mathbf{z}} = \\mathrm{softmax}\\left(\\frac{\\log{\\boldsymbol{\\pi}} + \\mathbf{G}}{\\tau}\\right),$$\nwhere $G_i \\sim \\mathrm{Gumbel}(0, 1)$.\nThe authors propose an alternative family of distributions that works by transforming Gaussian noise $\\boldsymbol{\\epsilon}$ through invertible transformation onto the simplex. In particular, map $\\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$ to simplex, using invertible $g(\\cdot, \\tau)$ with temperature $\\tau$:\n$$ \\begin{aligned} \\mathbf{y} \u0026= \\boldsymbol{\\mu} + \\mathrm{diag}(\\boldsymbol{\\sigma}) \\boldsymbol{\\epsilon},\\ \\tilde{\\mathbf{z}} \u0026= g(\\mathbf{y}, \\tau) = \\mathrm{softmax}_{++}(\\mathbf{y/\\tau}) \\end{aligned} $$\nThus, this is one more relaxation of Categorical distribution.\nHard Concrete (Louizos et al. 2018 The relaxed Bernoulli method can be viewed from another angle, if we consider it in the following form:\n$$ \\begin{aligned} s \u0026\\sim q(s | \\phi),\\ z \u0026= \\min (1, \\max (0, s)), \\end{aligned} $$\nwhere the distribution $q(s | \\phi)$ is normal $\\mathcal{N}(\\mu, \\sigma^2)$. The idea of Hard Concrete is to 1) consider a Gumbel-Softmax relaxation $q(s | \\phi) = \\mathrm{GS}(s | \\phi)$ with parameters $\\phi = (\\log \\alpha, \\tau)$; 2) stretch it from $(0, 1)$ to the wider interval $(\\gamma, \\zeta)$, with $\\gamma \u003c 0$ and $\\zeta \u003e 1$; and then 3) apply a hard-sigmoid on its random samples.\n$$ \\begin{aligned} s \u0026= \\sigma\\left( (g + \\log \\alpha) / \\tau \\right), \\quad g \\sim \\mathrm{Gumbel}(0, 1),\\ \\bar{s} \u0026= s (\\zeta - \\gamma) + \\gamma,\\ z \u0026= \\min (1, \\max (0, \\bar{s})). \\end{aligned} $$\nThis distribution provides a Bernoulli variable relaxation, applying hard-sigmoid technique to make two delta peaks at zero and one.\nClosed-form Laplace Bridge (Hobbhahn et al. 2020) In this and the next sections we consider quite another approaches used for discrete variables, but not relaxation actually. This one, closed-form Laplace Bridge, is an approach of approximating Dirichlet distribution with Logistic-Normal, and vice versa.\nWhy should we consider it? Indeed, these two distributions lies on the simplex and it is natural decision to find the parameters to match each of them with particular one.\nIn particular, the analytic map from the Dirichlet distribution parameter $\\boldsymbol{\\alpha} \\in \\mathbb{R}_{+}^{K}$ to the parameters of the Gaussian $\\boldsymbol{\\mu} \\in \\mathbb{R}^{K}$ and symmetric positive definite $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{K \\times K}$ is given by\n$$ \\begin{aligned} \\mu_i \u0026= \\log \\alpha_i - \\frac{1}{K} \\sum_{k=1}^{K} \\log \\alpha_k,\\ \\Sigma_{ij} \u0026= \\delta_{ij} \\frac{1}{\\alpha_i} - \\frac{1}{K} \\left( \\frac{1}{\\alpha_i} + \\frac{1}{\\alpha_j} - \\frac{1}{K} \\sum_{k=1}^{K} \\frac{1}{\\alpha_k} \\right), \\end{aligned} $$\nand the pseudo-inverse of this one, which maps the Gaussian parameters to those of the Dirichlet as\n$$ \\alpha_k = \\frac{1}{\\Sigma_{kk}} \\left( 1 - \\frac{2}{K} + \\frac{e^{\\mu_k}}{K^2} \\sum_{l=1}^{K} e^{-\\mu_l} \\right). $$\nAnd this is what is called Laplace Bridge between Dirichlet and Logistic-Normal distributions.\nImplementation (see our GitHub for details) In this section we describe our package design. The most famous Python probabilistic libraries with a built-in differentiation engine are PyTorch and Pyro. Thus, we implement the relaxit library consistently with both of them. Specifically, we\nTake a base class for PyTorch-compatible distributions with Pyro support TorchDistribution, for which we refer to this page on documentation. Inherent each of the considered relaxed distributions from this TorchDistribution. Implement batch_shape and event_shape properties that defines the distribution samples shapes. Implement rsample() and log_prob() methods as key two of the proposed algorithms. These methods are responsible for sample with reparameterization trick and log-likelihood computing respectively. For closed-form Laplace Bridge between Dirichlet and Logistic-Normal distributions we extend the base PyTorch KL-divergence method with one more realization. We also implement a LogisticNormalSoftmax distribution, which is a transformed distribution from the Normal one. In contrast to original LogisticNormal from Pyro or PyTorch, this one uses SoftmaxTransform, instead of StickBreakingTransform that allows us to remain in the same dimensionality.\nDemo Our demo code is available at this link. For demonstration purposes, we divide our algorithms in three1 different groups. Each group relates to the particular experiment:\nLaplace Bridge between Dirichlet and Logistic-Normal distributions; REINFORCE; Other relaxation methods. Laplace Bridge. This part relates to the demonstation of closed-form Laplace Bridge between Dirichlet and Logistic-Normal distributions. We subsequently 1) initialize a Dirichlet distribution with random parameters; 2) approximate it with a Logistic-Normal distribution; 3) approximate obtained Logistic-Normal distribution with Dirichlet one.\nDirichlet (with random parameters) Logistic-Normal (approximation to Dirichlet) Dirichlet (approximation to obtained Logistic-Normal) REINFORCE in Acrobot environment. In this part we train an Agent in the Acrobot environment, using REINFORCE to make optimization steps.\nVAE with discrete latents. All the other 6 algorithms are used to train a VAE with discrete latents. Each of the discussed relaxation techniques allows us to learn the latent space with the corresponding distribution. All implemented distributions have a similar structure, so we chose one distribution for demonstration and conducted a number of experiments with it. Correlated Relaxed Bernoulli was chosen as a demonstration method. This method generates correlated gate vectors from a multivariate Bernoulli distribution using a Gaussian copula. We define the parameters $\\pi$, $R$, and $\\tau$ as follows:\nTensor $\\pi$, representing the probabilities of the Bernoulli distribution, with an event shape of 3 and a batch size of 2: $$ \\pi = \\begin{bmatrix} 0.2 \u0026 0.4 \u0026 0.4 \\ 0.3 \u0026 0.5 \u0026 0.2 \\end{bmatrix} $$\nCorrelation matrix $R$ for the Gaussian copula: $$ R = \\begin{bmatrix} 1.0 \u0026 0.5 \u0026 0.3 \\ 0.5 \u0026 1.0 \u0026 0.7 \\ 0.3 \u0026 0.7 \u0026 1.0 \\end{bmatrix} $$\nTemperature hyperparameter $\\tau = 0.1$ Finally, after training we obtained reconstruction and sampling results for a MNIST dataset that we provide below. We see that VAE has learned something adequate, which means that the reparameterization is happening correctly. For the rest of the methods, VAE are also implemented, which you can get engaged using scripts in the demo experiments directory.\nFig. 2. Variational Autoencoder (VAE) with discrete Correlated Relaxed Bernoulli latents. Reconstruction. Fig. 3. Variational Autoencoder (VAE) with discrete Correlated Relaxed Bernoulli latents. Sampling. Conclusion In summary, Just Relax It is a powerful tool for researchers and practitioners working with discrete variables in neural networks. By offering a comprehensive set of relaxation techniques, our library aims to make the optimization process more efficient and accessible. We encourage you to explore our library, try out the demo, and contribute to its development. Together, we can push the boundaries of what is possible with discrete variable relaxation in machine learning.\nThank you for reading, and happy coding!\nDaniil Dorin, Igor Ignashin, Nikita Kiselev, Andrey Veprikov\nReferences We also implement REINFORCE algorithm as a score function estimator alternative for our relaxation methods that are inherently pathwise derivative estimators. This one is implemented only for demo experiments and is not included into the source code of package. ↩︎\n",
  "wordCount" : "2652",
  "inLanguage": "en",
  "image":"http://localhost:1313/overview.png","datePublished": "2024-12-07T00:00:00Z",
  "dateModified": "2024-12-07T00:00:00Z",
  "author":[{
    "@type": "Person",
    "name": "Daniil Dorin"
  }, {
    "@type": "Person",
    "name": "Igor Ignashin"
  }, {
    "@type": "Person",
    "name": "Nikita Kiselev"
  }, {
    "@type": "Person",
    "name": "Andrey Veprikov"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/projects/relaxit/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Nikita Kiselev",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js" integrity="sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"
  onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          delimiters: [
            {left: '$$', right: '$$', display: true},
            {left: '$', right: '$', display: false},
            {left: "\\begin{equation}", right: "\\end{equation}", display: true},
            {left: "\\begin{equation*}", right: "\\end{equation*}", display: true},
            {left: "\\begin{align}", right: "\\end{align}", display: true},
            {left: "\\begin{align*}", right: "\\end{align*}", display: true},
            {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
            {left: "\\begin{gather}", right: "\\end{gather}", display: true},
            {left: "\\begin{CD}", right: "\\end{CD}", display: true},
          ],
          throwOnError : false
        });
    });
</script>
 


</head>

<body class="" id="top">

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Nikita Kiselev">
                <img src="http://localhost:1313/favicon.ico" alt="" aria-label="logo"
                    height="18"
                    width="18">Nikita Kiselev</a>
            <div class="logo-switches">
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/publications/" title="Publications">
                    <span>Publications</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/talks/" title="Talks">
                    <span>Talks</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/projects/" title="Projects">
                    <span>Projects</span>
                </a>
            </li>
            <li>
                <a href="https://latexonline.cc/compile?git=https://github.com/kisnikser/CV&amp;target=resume.tex&amp;command=xelatex&amp;force=true" title="Resume">
                    <span>Resume</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
            <li>
                <a href="https://latexonline.cc/compile?git=https://github.com/kisnikser/CV&amp;target=cv.tex&amp;command=xelatex&amp;force=true" title="CV">
                    <span>CV</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/archive/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
        </ul>
    </nav>
</header>

    <main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Just Relax It! Leveraging relaxation for discrete variables optimization
    </h1>
    <div class="post-meta"><span title='2024-12-07 00:00:00 +0000 UTC'>December 2024</span>&nbsp;&middot;&nbsp;Daniil Dorin,&thinsp;Igor Ignashin,&thinsp;Nikita Kiselev,&thinsp;Andrey Veprikov

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><nav id="TableOfContents">
  <ul>
    <li><a href="#introduction">Introduction</a>
      <ul>
        <li><a href="#vae-example">VAE example</a></li>
        <li><a href="#other-relaxation-methods">Other relaxation methods</a></li>
      </ul>
    </li>
    <li><a href="#algorithms">Algorithms</a>
      <ul>
        <li><a href="#relaxed-bernoulli-yamada-lindenbaum-et-al-2018httpsarxivorgabs181004247">Relaxed Bernoulli (<a href="https://arxiv.org/abs/1810.04247">Yamada, Lindenbaum et al. 2018</a>)</a></li>
        <li><a href="#correlated-relaxed-bernoulli-lee-imrie-et-al-2022httpsopenreviewnetpdfidodfvtxzpox">Correlated relaxed Bernoulli (<a href="https://openreview.net/pdf?id=oDFvtxzPOx">Lee, Imrie et al. 2022</a>)</a></li>
        <li><a href="#gumbel-softmax-top-k-kool-et-al-2019httpsarxivorgpdf190306059">Gumbel-Softmax TOP-K (<a href="https://arxiv.org/pdf/1903.06059">Kool et al. 2019</a>)</a></li>
        <li><a href="#stochastic-times-smooth-bengio-et-al-2013httpsciteseerxistpsuedudocumentrepidrep1typepdfdoi62c76ca0b2790c34e85ba1cce09d47be317c7235">Stochastic Times Smooth (<a href="https://citeseerx.ist.psu.edu/document?repid=rep1&amp;type=pdf&amp;doi=62c76ca0b2790c34e85ba1cce09d47be317c7235">Bengio et al. 2013</a>)</a></li>
        <li><a href="#invertible-gaussian-potapczynski-et-al-2019httpsarxivorgabs191209588">Invertible Gaussian (<a href="https://arxiv.org/abs/1912.09588">Potapczynski et al. 2019</a>)</a></li>
        <li><a href="#hard-concrete-louizos-et-al-2018httpsarxivorgabs171201312">Hard Concrete (<a href="https://arxiv.org/abs/1712.01312">Louizos et al. 2018</a></a></li>
        <li><a href="#closed-form-laplace-bridge-hobbhahn-et-al-2020httpsarxivorgabs200301227">Closed-form Laplace Bridge (<a href="https://arxiv.org/abs/2003.01227">Hobbhahn et al. 2020</a>)</a></li>
      </ul>
    </li>
    <li><a href="#implementation-see-our-githubhttpsgithubcomintsystemsrelaxit-for-details">Implementation (see our <a href="%5B%5D(https://github.com/intsystems/relaxit)">GitHub</a> for details)</a></li>
    <li><a href="#demo">Demo</a></li>
    <li><a href="#conclusion">Conclusion</a></li>
    <li><a href="#references">References</a></li>
  </ul>
</nav>
        </div>
    </details>
</div>

  <div class="post-content"><p><img loading="lazy" src="overview.png" alt="Overview"  />
</p>
<p>In this blog-post we present our Python library <a href="https://github.com/intsystems/discrete-variables-relaxation" target="_blank">&ldquo;Just Relax It&rdquo;</a> (or <code>relaxit</code>) designed to streamline the optimization of discrete probability distributions in neural networks, offering a suite of advanced relaxation techniques compatible with PyTorch.</p>
<h2 id="introduction">Introduction</h2>
<p>Recent development of generative models, e.g. VAE and Diffusion Models, has driven relevant mathematical tools.
Any generative model contains some source of randomness to make new objects.
This randomness represents a certain probability distribution, from which random variables are sampled.
Thus, the task of training a generative model is often comes down to optimization of such distribution parameters.</p>
<p>Pioneering generative models work with <strong>continous</strong> distributions like Normal one.
However, for some modalities, e.g. texts or graphs, it is quite natural to use <strong>discrete</strong> distributions – Bernoulli, Categorical, etc.</p>
<p>Thus, we present our new Python library <a href="https://github.com/intsystems/discrete-variables-relaxation" target="_blank">&ldquo;Just Relax It&rdquo;</a> that combines the best techniques for relaxing discrete distributions (we will explain what that means later) into an easy-to-use package. And it is compatible with PyTorch!</p>
<p>We start with a basic example that shows how parameter optimization typically happens for continuous distributions, then we move on smoothly to the case of discrete distributions. After that, we talk about the main relaxation techniques used in our library and make a demo of training a VAE with discrete latent variables.</p>
<h3 id="vae-example">VAE example</h3>
<figure>
    <img loading="lazy" src="demo.png"/> <figcaption>
            Fig. 1. Variational Autoencoder (VAE) architecture.
        </figcaption>
</figure>

<p>The original VAE (<a href="https://arxiv.org/abs/1312.6114" target="_blank">Kingma &amp; Welling, 2014</a>) consists of two parts:</p>
<ol>
<li>Encoder $q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x})$, which is represented by a neural network $g_{\boldsymbol{\phi}}(\mathbf{x})$ that outputs parameters of the latent Gaussian distribution;</li>
<li>Decoder $p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z})$, which is represented by a neural network $f_{\boldsymbol{\theta}}(\mathbf{z})$ that outputs parameters of the sample distribution (typically Gaussian or Bernoulli).</li>
</ol>
<p>The math behind training a VAE is not obvious actually, so we will just focus on the ELBO (evidence lower bound), which needs to be maximized w.r.t. the parameters of the encoder and decoder:
$$
\mathcal{L}<em>{\boldsymbol{\phi}, \boldsymbol{\theta}}(\mathbf{x}) = \mathbb{E}</em>{q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x})} \log p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z}) - KL(q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) | p(\mathbf{z})) \to \max_{\boldsymbol{\phi}, \boldsymbol{\theta}}.
$$</p>
<p>During the <strong>M-step</strong>, we gonna derive the unbiased estimator for the gradient $\nabla_{\boldsymbol{\theta}}\mathcal{L}_{\boldsymbol{\phi}, \boldsymbol{\theta}}(\mathbf{x})$:</p>
<p>$$
\begin{aligned}
\nabla_{\boldsymbol{\theta}}\mathcal{L}<em>{\boldsymbol{\phi}, \boldsymbol{\theta}}(\mathbf{x})
&amp;= \textcolor{blue}{\nabla</em>{\boldsymbol{\theta}}} \int q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) \log p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z}) d\mathbf{z} \
&amp;= \int q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) \textcolor{blue}{\nabla_{\boldsymbol{\theta}}} \log p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z}) d\mathbf{z} \
&amp;\approx \nabla_{\boldsymbol{\theta}} \log p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z}^<em>), \quad \mathbf{z}^</em> \sim q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}),
\end{aligned}
$$</p>
<p>where the last approximation is a Monte-Carlo sampling estimator.</p>
<p>However, on the <strong>E-step</strong> it is quite tricky to get unbiased estimator for the gradient $$\nabla_{\boldsymbol{\phi}}\mathcal{L}_{\boldsymbol{\phi}, \boldsymbol{\theta}}(\mathbf{x}).$$</p>
<p>As density function $q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x})$ depends on the parameters $\boldsymbol{\phi}$, it is impossible to use the Monte-Carlo estimation:</p>
<p>$$
\begin{aligned}
\nabla_{\boldsymbol{\phi}}\mathcal{L}<em>{\boldsymbol{\phi}, \boldsymbol{\theta}}(\mathbf{x})
&amp;= \textcolor{blue}{\nabla</em>{\boldsymbol{\phi}}} \int q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) \log p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z}) d\mathbf{z} - \nabla_{\boldsymbol{\phi}} KL(q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) | p(\mathbf{z})) \
&amp;\textcolor{red}{\neq} \int q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) \textcolor{blue}{\nabla_{\boldsymbol{\theta}}} \log p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z}) d\mathbf{z} - \nabla_{\boldsymbol{\phi}} KL(q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) | p(\mathbf{z})), \
\end{aligned}
$$</p>
<p>and this is the moment where the <strong>reparameterization trick</strong> arises, we reparameterize the outputs of the <strong>encoder</strong>:</p>
<p>$$
\begin{aligned}
\nabla_{\boldsymbol{\phi}} \int \textcolor{blue}{q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x})} \log p_{\boldsymbol{\theta}}(\mathbf{x}|\textcolor{OliveGreen}{\mathbf{z}}) d\mathbf{z}
&amp;= \int \textcolor{blue}{p(\boldsymbol{\epsilon})} \nabla_{\boldsymbol{\phi}} \log p_{\boldsymbol{\theta}}(\mathbf{x}|\textcolor{OliveGreen}{\mathbf{g}<em>{\boldsymbol{\phi}}(\mathbf{x}, \boldsymbol{\epsilon})}) d\boldsymbol{\epsilon} \
&amp;\approx \nabla</em>{\boldsymbol{\phi}} \log p_{\boldsymbol{\theta}}(\mathbf{x}|\textcolor{OliveGreen}{\boldsymbol{\sigma}<em>{\boldsymbol{\phi}}(\mathbf{x})} \odot \textcolor{blue}{\boldsymbol{\epsilon}^*} + \textcolor{OliveGreen}{\boldsymbol{\mu}</em>{\boldsymbol{\phi}}(\mathbf{x})}), \quad \textcolor{blue}{\boldsymbol{\epsilon}^*} \sim \mathcal{N}(0, \mathbf{I}),
\end{aligned}
$$</p>
<p>so we move the randomness to the $\boldsymbol{\epsilon} \sim p(\boldsymbol{\epsilon})$, and use the deterministic transform $$\mathbf{z} = \mathbf{g}<em>{\boldsymbol{\phi}}(\mathbf{x}, \boldsymbol{\epsilon})$$ in order to get unbiased gradient. It also needs to be mentioned that normal assumptions for $q</em>{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x})$ and $p(\mathbf{z})$ allows us to compute $KL$ analytically and thus calculate the gradient $\nabla_{\boldsymbol{\phi}}KL(q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) | p(\mathbf{z}))$.</p>
<p>The above example gives us an understanding of a crucial reparameterization trick, which allows us to get unbiased gradient estimations for the continuous latent space in VAE model. But actually discrete representations $\mathbf{z}$ are potentially a more natural fit for many of the modalities (like texts or images), which moves us to the <strong>discrete VAE latentes</strong>. Therefore</p>
<ul>
<li>Our encoder should output discrete distribution;</li>
<li>We need the analogue of the reparameterization trick for the discrete distribution;</li>
<li>Our decoder should input discrete random variable.</li>
</ul>
<p>The classical solution for the discrete variables reparameterization trick is <strong>Gumbel-Softmax</strong> (<a href="https://arxiv.org/abs/1611.01144" target="_blank">Jang et al. 2017</a>) or <strong>Concrete relaxation</strong> (<a href="https://arxiv.org/abs/1611.00712" target="_blank">Maddison et al. 2017</a>).</p>
<h4 id="gumbel-distribution">Gumbel distribution</h4>
<p>$$ g \sim \mathrm{Gumbel}(0, 1) \quad \Leftrightarrow \quad g = -\log( - \log u), \quad u \sim \mathrm{Uniform}[0, 1] $$</p>
<h4 id="theorem-gumbel-max-trick">Theorem (Gumbel-Max trick)</h4>
<p>Let $g_k \sim \mathrm{Gumbel}(0, 1)$ for $k = 1, \ldots, K$. Then a discrete random variable</p>
<p>$$ c = \arg\max_k [\log \pi_k + g_k] $$</p>
<p>has a categorical distribution $c \sim \mathrm{Categorical}(\boldsymbol{\pi})$.</p>
<ul>
<li>We could sample from the discrete distribution using Gumbel-Max reparameterization;</li>
<li>Here parameters and random variable sampling are separated (reparameterization trick);</li>
<li><strong>Problem:</strong> we still have non-differentiable $\arg\max$ operation.</li>
</ul>
<h4 id="gumbel-softmax-relaxation">Gumbel-Softmax relaxation</h4>
<p>$$ \hat{\mathbf{c}} = \mathrm{softmax}\left( \frac{\log q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) + \mathbf{g}}{\tau} \right) $$</p>
<p>Here $\tau$ is a temperature parameters. Now we have differentiable operation, but the gradient estimator is biased now. However, if $\tau \to 0$, then the estimation becomes more and more accurate.</p>
<h3 id="other-relaxation-methods">Other relaxation methods</h3>
<p>So far, we have talked about one possible example of a discrete variable relaxation (VAE with discrete latent variables) and the classical approach to solving this problem. However, the Gumbel-Softmax relaxation was actually proposed a long time ago. There are actually many other relaxation techniques that can provide more flexible and accurate (and even unbiased) gradient estimates. The rest of our blog-post will focus on cutting-edge relaxation techniques and how we built a Python library that uses them, which works with the PyTorch framework to train neural networks efficiently.</p>
<h2 id="algorithms">Algorithms</h2>
<p>In this section, we provide a short description for each of the implemented methods. We can generalize them as follows. Suppose that $x$ is a random variable, $f$ if a function (say, the loss function), and we are interested in computing $\frac{\partial}{\partial \theta} \mathbb{E}_{x}\left[ f(x) \right]$. It is quite natural decision because typical ML problem looks like this. So, two different ideas exist:</p>
<ul>
<li><em>Score function</em> (SF) estimator. In this case, we are given a parameterized probability distribution $x \sim p(\cdot; \theta)$ and use</li>
</ul>
<p>$$ \frac{\partial}{\partial \theta} \mathbb{E}<em>{x}\left[ f(x) \right] = \mathbb{E}</em>{x} \left[ f(x) \frac{\partial}{\partial \theta} \log p(x; \theta) \right]. $$</p>
<ul>
<li><em>Pathwise derivative</em> (PD) estimator. In this case $x$ is a determinisitc, differentiable function of $\theta$ and another random variable $z$, i.e. we can write $x(z, \theta)$:</li>
</ul>
<p>$$ \frac{\partial}{\partial \theta} \mathbb{E}<em>{x}\left[ f(x(z, \theta)) \right] = \mathbb{E}</em>{z} \left[ \frac{\partial}{\partial \theta} f(x(z, \theta)) \right]. $$</p>
<p>The latter one we have seen previously in the VAE example! A sample $x$ from $\mathcal{N}(\mu, \sigma^2)$ can be obtained by sampling $z$ from the standard normal distribution $\mathcal{N}(0, 1)$ and then transforming it using $x(z, \theta) = \sigma z + \mu$. And this is called reparameterization trick.</p>
<p>However, when $x$ is a discrete variable, it is quite tricky to make a pathwise derivative estimator, i.e. to reparameterize the discrete distribution. And this is the moment of relaxation! We replace $x$ with a continuous relaxation $x(z, \theta) \approx x_{\tau}(z, \theta)$, where $\tau &gt; 0$ is a temperature that controls the tightness of the relaxaton (at low temperatues, the relaxation is nearly high).</p>
<h3 id="relaxed-bernoulli-yamada-lindenbaum-et-al-2018httpsarxivorgabs181004247">Relaxed Bernoulli (<a href="https://arxiv.org/abs/1810.04247" target="_blank">Yamada, Lindenbaum et al. 2018</a>)</h3>
<p>The reparameterization trick is inspired by the idea of stochastic gates and aims to approximate a Bernoulli random variable in a more relaxed manner. This technique involves drawing a random variable, denoted as $\epsilon$, from a normal distribution with a mean of 0 and a variance of $\sigma^2$, where $\sigma$ is a fixed parameter. The random variable $\epsilon$ is then used to compute $z$ as follows:</p>
<p>$$
\begin{aligned}
\epsilon &amp;\sim \mathcal{N}(0, \sigma^2),\
z &amp;= \min (1, \max (0, \mu + \epsilon)),
\end{aligned}
$$</p>
<p>where $\mu$ is a learnable parameter that can be tuned during the training process. This transformation ensures that the resulting $z$ value is bounded between 0 and 1, thereby <strong>relaxing the Bernoulli distribution</strong>.</p>
<h3 id="correlated-relaxed-bernoulli-lee-imrie-et-al-2022httpsopenreviewnetpdfidodfvtxzpox">Correlated relaxed Bernoulli (<a href="https://openreview.net/pdf?id=oDFvtxzPOx" target="_blank">Lee, Imrie et al. 2022</a>)</h3>
<p>This method generates correlated gate vectors from a multivariate Bernoulli distribution using a Gaussian copula:</p>
<p>$$C_R(U_1, \ldots, U_p) = \Phi_R(\Phi^{-1}(U_1), \ldots, \Phi^{-1}(U_p)),$$</p>
<p>where $\Phi_R$ is the joint CDF of a multivariate Gaussian distribution with correlation matrix $R$, and $\Phi^{-1}$ is the inverse CDF of the standard univariate Gaussian distribution.</p>
<p>The gate vector $m$ is generated as:</p>
<p>$$m_k =
\begin{cases}
1, &amp; \text{if } U_k \leq \pi_k, \
0, &amp; \text{if } U_k &gt; \pi_k,
\end{cases}\quad k = 1, \ldots, p,
$$</p>
<p>where $U_k$ are correlated random variables preserving the input feature correlations.</p>
<p>For differentiability, a continuous relaxation is applied:</p>
<p>$$m_k = \sigma \left( \frac{1}{\tau} \left( \log \frac{U_k}{1 - U_k} + \log \frac{\pi_k}{1 - \pi_k} \right) \right),$$</p>
<p>where $\sigma(x) = \frac{1}{1 + \exp(-x)}$ is the sigmoid function, and $\tau$ is a temperature hyperparameter. Thus, the <strong>Bernolli distribution relaxes</strong>.</p>
<h3 id="gumbel-softmax-top-k-kool-et-al-2019httpsarxivorgpdf190306059">Gumbel-Softmax TOP-K (<a href="https://arxiv.org/pdf/1903.06059" target="_blank">Kool et al. 2019</a>)</h3>
<p>Suppose we want to get $K$ samples without replacement (i.e., not repeating) according to the Categorical distribution with probabilities $\boldsymbol{\pi}$. Similar to the Gumbel-Max method,  let $g_k \sim \mathrm{Gumbel}(0, 1)$ for $k = 1, \ldots, K$, then the Gumbel-Max-Top$K$ Theorem says, that the values of the form</p>
<p>$$c_1, \ldots , c_K = \text{Arg}\underset{k}{\text{top}}K [ \log\pi_k + g_k]$$</p>
<p>have the $\mathrm{Categorical}(\boldsymbol{\pi})$ distribution without replacement.</p>
<p>This approach has all the same pros and cons as the classical Gumbel-Max trick, however, they can be fixed with the Gumbel-Softmax relaxation using a simple loop:</p>
<p>$$ Algorithm $$</p>
<p>Therefore, this method allows us to <strong>relax the Categorical distribution</strong>.</p>
<h3 id="stochastic-times-smooth-bengio-et-al-2013httpsciteseerxistpsuedudocumentrepidrep1typepdfdoi62c76ca0b2790c34e85ba1cce09d47be317c7235">Stochastic Times Smooth (<a href="https://citeseerx.ist.psu.edu/document?repid=rep1&amp;type=pdf&amp;doi=62c76ca0b2790c34e85ba1cce09d47be317c7235" target="_blank">Bengio et al. 2013</a>)</h3>
<p>The Stochastic Times Smooth distribution can be written as follows</p>
<p>$$
\begin{aligned}
p_i &amp;= \sigma(a_i), \
b_i &amp;\sim \text{Binomial}(\sqrt{p_i}), \
h_i &amp;= b_i \sqrt{p_i},
\end{aligned}
$$</p>
<p>where $a_i$ is a parameter of this distribution. This one provides a <strong>Bernoulli distribution relaxation</strong>.</p>
<h3 id="invertible-gaussian-potapczynski-et-al-2019httpsarxivorgabs191209588">Invertible Gaussian (<a href="https://arxiv.org/abs/1912.09588" target="_blank">Potapczynski et al. 2019</a>)</h3>
<p>The idea of this method is to remove interpretability of parameters in Gumbel-Softmax relaxation, and achieve then higher quality. Namely, the goal of Gumbel-Softmax relaxation is to relax $\mathbf{z} \sim \mathrm{Cat}(\boldsymbol{\pi})$ proposing temperature parameter $\tau \to 0$, which concentrates mass on vertices:</p>
<p>$$\tilde{\mathbf{z}} = \mathrm{softmax}\left(\frac{\log{\boldsymbol{\pi}} + \mathbf{G}}{\tau}\right),$$</p>
<p>where $G_i \sim \mathrm{Gumbel}(0, 1)$.</p>
<p>The authors propose an alternative family of distributions that works by transforming Gaussian noise $\boldsymbol{\epsilon}$ through invertible transformation onto the simplex. In particular, map $\mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})$ to simplex, using invertible  $g(\cdot, \tau)$ with temperature $\tau$:</p>
<p>$$
\begin{aligned}
\mathbf{y} &amp;= \boldsymbol{\mu} + \mathrm{diag}(\boldsymbol{\sigma}) \boldsymbol{\epsilon},\
\tilde{\mathbf{z}} &amp;= g(\mathbf{y}, \tau) = \mathrm{softmax}_{++}(\mathbf{y/\tau})
\end{aligned}
$$</p>
<p>Thus, this is one more <strong>relaxation of Categorical distribution</strong>.</p>
<h3 id="hard-concrete-louizos-et-al-2018httpsarxivorgabs171201312">Hard Concrete (<a href="https://arxiv.org/abs/1712.01312" target="_blank">Louizos et al. 2018</a></h3>
<p>The relaxed Bernoulli method can be viewed from another angle, if we consider it in the following form:</p>
<p>$$
\begin{aligned}
s &amp;\sim q(s | \phi),\
z &amp;= \min (1, \max (0, s)),
\end{aligned}
$$</p>
<p>where the distribution $q(s | \phi)$ is normal $\mathcal{N}(\mu, \sigma^2)$. The idea of Hard Concrete is to 1) consider a Gumbel-Softmax relaxation $q(s | \phi) = \mathrm{GS}(s | \phi)$ with parameters $\phi = (\log \alpha, \tau)$; 2) stretch it from $(0, 1)$ to the wider interval $(\gamma, \zeta)$, with $\gamma &lt; 0$ and $\zeta &gt; 1$; and then 3) apply a hard-sigmoid on its random samples.</p>
<p>$$
\begin{aligned}
s &amp;= \sigma\left( (g + \log \alpha) / \tau \right), \quad g \sim \mathrm{Gumbel}(0, 1),\
\bar{s} &amp;= s (\zeta - \gamma) + \gamma,\
z &amp;= \min (1, \max (0, \bar{s})).
\end{aligned}
$$</p>
<p>This distribution provides a <strong>Bernoulli variable relaxation</strong>, applying hard-sigmoid technique to make two delta peaks at zero and one.</p>
<h3 id="closed-form-laplace-bridge-hobbhahn-et-al-2020httpsarxivorgabs200301227">Closed-form Laplace Bridge (<a href="https://arxiv.org/abs/2003.01227" target="_blank">Hobbhahn et al. 2020</a>)</h3>
<p>In this and the next sections we consider quite another approaches used for discrete variables, but not relaxation actually. This one, closed-form Laplace Bridge, is an approach of approximating Dirichlet distribution with Logistic-Normal, and vice versa.</p>
<p>Why should we consider it? Indeed, these two distributions lies on the simplex and it is natural decision to find the parameters to match each of them with particular one.</p>
<p>In particular, the analytic map from the Dirichlet distribution parameter $\boldsymbol{\alpha} \in \mathbb{R}_{+}^{K}$ to the parameters of the Gaussian $\boldsymbol{\mu} \in \mathbb{R}^{K}$ and symmetric positive definite $\boldsymbol{\Sigma} \in \mathbb{R}^{K \times K}$ is given by</p>
<p>$$
\begin{aligned}
\mu_i &amp;= \log \alpha_i - \frac{1}{K} \sum_{k=1}^{K} \log \alpha_k,\
\Sigma_{ij} &amp;= \delta_{ij} \frac{1}{\alpha_i} - \frac{1}{K} \left( \frac{1}{\alpha_i} + \frac{1}{\alpha_j} - \frac{1}{K} \sum_{k=1}^{K} \frac{1}{\alpha_k} \right),
\end{aligned}
$$</p>
<p>and the pseudo-inverse of this one, which maps the Gaussian parameters to those of the Dirichlet as</p>
<p>$$
\alpha_k = \frac{1}{\Sigma_{kk}} \left( 1 - \frac{2}{K} + \frac{e^{\mu_k}}{K^2} \sum_{l=1}^{K} e^{-\mu_l} \right).
$$</p>
<p>And this is what is called <strong>Laplace Bridge between Dirichlet and Logistic-Normal distributions</strong>.</p>
<h2 id="implementation-see-our-githubhttpsgithubcomintsystemsrelaxit-for-details">Implementation (see our <a href="[]%28https://github.com/intsystems/relaxit%29">GitHub</a> for details)</h2>
<p>In this section we describe our package design. The most famous Python probabilistic libraries with a built-in differentiation engine are <a href="https://pytorch.org/docs/stable/index.html" target="_blank">PyTorch</a> and <a href="https://docs.pyro.ai/en/dev/index.html" target="_blank">Pyro</a>. Thus, we implement the <code>relaxit</code> library consistently with both of them. Specifically, we</p>
<ol>
<li>Take a base class for PyTorch-compatible distributions with Pyro support <code>TorchDistribution</code>, for which we refer to <a href="https://docs.pyro.ai/en/dev/distributions.html#torchdistribution" target="_blank">this page</a> on documentation.</li>
<li>Inherent each of the considered relaxed distributions from this <code>TorchDistribution</code>.</li>
<li>Implement <code>batch_shape</code> and <code>event_shape</code> properties that defines the distribution samples shapes.</li>
<li>Implement <code>rsample()</code> and <code>log_prob()</code> methods as key two of the proposed algorithms. These methods are responsible for sample with reparameterization trick and log-likelihood computing respectively.</li>
</ol>
<p>For closed-form Laplace Bridge between Dirichlet and Logistic-Normal distributions we extend the base PyTorch KL-divergence method with one more realization. We also implement a <code>LogisticNormalSoftmax</code> distribution, which is a transformed distribution from the <code>Normal</code> one. In contrast to original <code>LogisticNormal</code> from Pyro or PyTorch, this one uses <code>SoftmaxTransform</code>, instead of <code>StickBreakingTransform</code> that allows us to remain in the same dimensionality.</p>
<h2 id="demo">Demo</h2>
<p>Our demo code is available at <a href="https://github.com/intsystems/discrete-variables-relaxation/tree/main/demo" target="_blank">this link</a>. For demonstration purposes, we divide our algorithms in three<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> different groups. Each group relates to the particular experiment:</p>
<ol>
<li>Laplace Bridge between Dirichlet and Logistic-Normal distributions;</li>
<li>REINFORCE;</li>
<li>Other relaxation methods.</li>
</ol>
<p><strong>Laplace Bridge.</strong> This part relates to the demonstation of closed-form Laplace Bridge between Dirichlet and Logistic-Normal distributions. We subsequently 1) initialize a Dirichlet distribution with random parameters; 2) approximate it with a Logistic-Normal distribution; 3) approximate obtained Logistic-Normal distribution with Dirichlet one.</p>
<table>
<thead>
<tr>
<th style="text-align:center">Dirichlet (with random parameters)</th>
<th style="text-align:center">Logistic-Normal (approximation to Dirichlet)</th>
<th style="text-align:center">Dirichlet (approximation to obtained Logistic-Normal)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img loading="lazy" src="laplace-bridge-1.png" alt="Closed-form Laplace Bridge demonstration"  />
</td>
<td style="text-align:center"><img loading="lazy" src="laplace-bridge-2.png" alt="Closed-form Laplace Bridge demonstration"  />
</td>
<td style="text-align:center"><img loading="lazy" src="laplace-bridge-3.png" alt="Closed-form Laplace Bridge demonstration"  />
</td>
</tr>
</tbody>
</table>
<p><strong>REINFORCE in Acrobot environment.</strong> In this part we train an Agent in the <a href="https://www.gymlibrary.dev/environments/classic_control/acrobot/" target="_blank">Acrobot environment</a>, using REINFORCE to make optimization steps.</p>
<p><strong>VAE with discrete latents.</strong> All the other 6 algorithms are used to train a VAE with discrete latents. Each of the discussed relaxation techniques allows us to learn the latent space with the corresponding distribution. All implemented distributions have a similar structure, so we chose one distribution for demonstration and conducted a number of experiments with it. <strong>Correlated Relaxed Bernoulli</strong> was chosen as a demonstration method. This method generates correlated gate vectors from a multivariate Bernoulli distribution using a Gaussian copula. We define the parameters $\pi$, $R$, and $\tau$ as follows:</p>
<ul>
<li>Tensor $\pi$, representing the probabilities of the Bernoulli distribution, with an event shape of 3 and a batch size of 2:</li>
</ul>
<p>$$
\pi = \begin{bmatrix}
0.2 &amp; 0.4 &amp; 0.4 \
0.3 &amp; 0.5 &amp; 0.2
\end{bmatrix}
$$</p>
<ul>
<li>Correlation matrix $R$ for the Gaussian copula:</li>
</ul>
<p>$$
R = \begin{bmatrix}
1.0 &amp; 0.5 &amp; 0.3 \
0.5 &amp; 1.0 &amp; 0.7 \
0.3 &amp; 0.7 &amp; 1.0
\end{bmatrix}
$$</p>
<ul>
<li>Temperature hyperparameter $\tau = 0.1$</li>
</ul>
<p>Finally, after training we obtained reconstruction and sampling results for a MNIST dataset that we provide below. We see that VAE has learned something adequate, which means that the reparameterization is happening correctly. For the rest of the methods, VAE are also implemented, which you can get engaged using scripts in the demo experiments directory.</p>
<p><figure>
    <img loading="lazy" src="correlated_bernoulli_reconstruction.png"/> <figcaption>
            Fig. 2. Variational Autoencoder (VAE) with discrete Correlated Relaxed Bernoulli latents. Reconstruction.
        </figcaption>
</figure>

<figure>
    <img loading="lazy" src="correlated_bernoulli_sample.png"/> <figcaption>
            Fig. 3. Variational Autoencoder (VAE) with discrete Correlated Relaxed Bernoulli latents. Sampling.
        </figcaption>
</figure>
</p>
<h2 id="conclusion">Conclusion</h2>
<p>In summary, <code>Just Relax It</code> is a powerful tool for researchers and practitioners working with discrete variables in neural networks. By offering a comprehensive set of relaxation techniques, our library aims to make the optimization process more efficient and accessible. We encourage you to explore our library, try out the demo, and contribute to its development. Together, we can push the boundaries of what is possible with discrete variable relaxation in machine learning.</p>
<p>Thank you for reading, and happy coding!</p>
<p><a href="https://github.com/DorinDaniil" target="_blank">Daniil Dorin</a>, <a href="https://github.com/ThunderstormXX" target="_blank">Igor Ignashin</a>, <a href="https://kisnikser.github.io/" target="_blank"><strong>Nikita Kiselev</strong></a>, <a href="https://github.com/Vepricov" target="_blank">Andrey Veprikov</a></p>
<h2 id="references">References</h2>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>We also implement REINFORCE algorithm as a score function estimator alternative for our relaxation methods that are inherently pathwise derivative estimators. This one is implemented only for demo experiments and is not included into the source code of package.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/relaxation/">Relaxation</a></li>
      <li><a href="http://localhost:1313/tags/gumbel-softmax/">Gumbel-Softmax</a></li>
      <li><a href="http://localhost:1313/tags/straight-through-estimator/">Straight-Through Estimator</a></li>
      <li><a href="http://localhost:1313/tags/python/">Python</a></li>
      <li><a href="http://localhost:1313/tags/library/">Library</a></li>
      <li><a href="http://localhost:1313/tags/package/">Package</a></li>
      <li><a href="http://localhost:1313/tags/pytorch/">PyTorch</a></li>
      <li><a href="http://localhost:1313/tags/pyro/">Pyro</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="http://localhost:1313/">Nikita Kiselev</a></span> ·     
    <span>
    Powered by 
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/pmichaillat/hugo-website/" rel="noopener" target="_blank">a modified version</a>
         of 
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>
</html>
