<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Just Relax It! Leveraging relaxation for discrete variables optimization | Nikita Kiselev</title>
<meta name="keywords" content="Relaxation, Gumbel-Softmax, Straight-Through Estimator, Python, Library, Package, PyTorch, Pyro">
<meta name="description" content="We release a cutting-edge Python library designed to streamline the optimization of discrete probability distributions in neural networks, offering a suite of advanced relaxation techniques compatible with PyTorch.">
<meta name="author" content="Daniil Dorin,&thinsp;Igor Ignashin,&thinsp;Nikita Kiselev,&thinsp;Andrey Veprikov">
<link rel="canonical" href="http://localhost:1313/projects/relaxit/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.177ebaef21802192b8610df8675c7cd36b682ac037b5d9bfad18dc908b59de1d.css" integrity="sha256-F3667yGAIZK4YQ34Z1x802toKsA3tdm/rRjckItZ3h0=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/projects/relaxit/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:title" content="Just Relax It! Leveraging relaxation for discrete variables optimization" />
<meta property="og:description" content="We release a cutting-edge Python library designed to streamline the optimization of discrete probability distributions in neural networks, offering a suite of advanced relaxation techniques compatible with PyTorch." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://localhost:1313/projects/relaxit/" />
<meta property="og:image" content="http://localhost:1313/overview.png" /><meta property="article:section" content="projects" />
<meta property="article:published_time" content="2024-12-07T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-12-07T00:00:00+00:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="http://localhost:1313/overview.png" />
<meta name="twitter:title" content="Just Relax It! Leveraging relaxation for discrete variables optimization"/>
<meta name="twitter:description" content="We release a cutting-edge Python library designed to streamline the optimization of discrete probability distributions in neural networks, offering a suite of advanced relaxation techniques compatible with PyTorch."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Projects",
      "item": "http://localhost:1313/projects/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Just Relax It! Leveraging relaxation for discrete variables optimization",
      "item": "http://localhost:1313/projects/relaxit/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Just Relax It! Leveraging relaxation for discrete variables optimization",
  "name": "Just Relax It! Leveraging relaxation for discrete variables optimization",
  "description": "We release a cutting-edge Python library designed to streamline the optimization of discrete probability distributions in neural networks, offering a suite of advanced relaxation techniques compatible with PyTorch.",
  "keywords": [
    "Relaxation", "Gumbel-Softmax", "Straight-Through Estimator", "Python", "Library", "Package", "PyTorch", "Pyro"
  ],
  "articleBody": " In this blog-post we present our Python library “Just Relax It” (or relaxit) designed to streamline the optimization of discrete probability distributions in neural networks, offering a suite of advanced relaxation techniques compatible with PyTorch.\nIntroduction Rapid advancement of generative models, such as Variational Autoencoders (VAEs) and Diffusion Models, has driven the development of relevant mathematical tools. Any generative model contains some source of randomness to make new objects. This randomness is represented by a probability distribution, from which random variables are sampled. Therefore, training a generative model often boils down to optimizing the parameters of this distribution.\nPioneering generative models typically work with continuous distributions like the Normal distribution. However, for some modalities, such as texts or graphs, it is more natural to use discrete distributions — Bernoulli, Categorical, etc.\nThus, we present our new Python library “Just Relax It” that combines the best techniques for relaxing discrete distributions (we will explain what that means later) into an easy-to-use package. And it is compatible with PyTorch!\nWe start with a basic Variational Autoencoder (VAE) example that shows how parameter optimization typically happens for continuous distributions, then we move on to the case of discrete distributions. After that, we overview relaxation methods used in our library and provide a demo of training a VAE with discrete latent variables.\nVAE Example We assume that you are already familiar with the concept of a Variational Autoencoder (VAE), at least at the level that it consists of two parts: an encoder and a decoder, and tries to approximate the distribution $p(\\mathbf{x})$ of data $\\mathbf{x}$ using latent variables $\\mathbf{z}$. If this is not the case, then we recommend that you to read a blog-post by Lilian Weng.\nFig. 1. Illustration of Variational Autoencoder (VAE) architecture. The original VAE (Kingma \u0026 Welling, 2014) has two main parts:\nEncoder $q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x})$: A neural network $g_{\\boldsymbol{\\phi}}(\\mathbf{x})$ that outputs parameters of the latent Gaussian distribution. Decoder $p_{\\boldsymbol{\\theta}}(\\mathbf{x}|\\mathbf{z})$: A neural network $f_{\\boldsymbol{\\theta}}(\\mathbf{z})$ that outputs parameters of the sample distribution (usually Gaussian or Bernoulli). Training a VAE involves maximizing the ELBO (evidence lower bound) with respect to the encoder and decoder parameters:\n$$ \\mathcal{L}_{\\boldsymbol{\\phi}, \\boldsymbol{\\theta}}(\\mathbf{x}) = \\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x})} \\log p_{\\boldsymbol{\\theta}}(\\mathbf{x}|\\mathbf{z}) - KL(q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x}) | p(\\mathbf{z})) \\to \\max_{\\boldsymbol{\\phi}, \\boldsymbol{\\theta}}. $$\nDuring the M-step, we derive an unbiased estimator for the gradient $\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}_{\\boldsymbol{\\phi}, \\boldsymbol{\\theta}}(\\mathbf{x})$:\n$$ \\begin{aligned} \\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}_{\\boldsymbol{\\phi}, \\boldsymbol{\\theta}}(\\mathbf{x}) \u0026= \\textcolor{blue}{\\nabla_{\\boldsymbol{\\theta}}} \\int q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x}) \\log p_{\\boldsymbol{\\theta}}(\\mathbf{x}|\\mathbf{z}) d\\mathbf{z} \\\\ \u0026= \\int q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x}) \\textcolor{blue}{\\nabla_{\\boldsymbol{\\theta}}} \\log p_{\\boldsymbol{\\theta}}(\\mathbf{x}|\\mathbf{z}) d\\mathbf{z} \\\\ \u0026\\approx \\nabla_{\\boldsymbol{\\theta}} \\log p_{\\boldsymbol{\\theta}}(\\mathbf{x}|\\mathbf{z}^*), \\quad \\mathbf{z}^* \\sim q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x}), \\end{aligned} $$\nwhere the last approximation uses Monte-Carlo sampling.\nHowever, during the E-step, getting an unbiased estimator for the gradient $\\nabla_{\\boldsymbol{\\phi}}\\mathcal{L}_{\\boldsymbol{\\phi}, \\boldsymbol{\\theta}}(\\mathbf{x})$ is tricky because the density function $q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x})$ depends on $\\boldsymbol{\\phi}$. This makes Monte-Carlo estimation impossible:\n$$ \\begin{aligned} \\nabla_{\\boldsymbol{\\phi}}\\mathcal{L}_{\\boldsymbol{\\phi}, \\boldsymbol{\\theta}}(\\mathbf{x}) \u0026= \\textcolor{blue}{\\nabla_{\\boldsymbol{\\phi}}} \\int q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x}) \\log p_{\\boldsymbol{\\theta}}(\\mathbf{x}|\\mathbf{z}) d\\mathbf{z} - \\nabla_{\\boldsymbol{\\phi}} KL(q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x}) | p(\\mathbf{z})) \\\\ \u0026\\textcolor{red}{\\neq} \\int q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x}) \\textcolor{blue}{\\nabla_{\\boldsymbol{\\phi}}} \\log p_{\\boldsymbol{\\theta}}(\\mathbf{x}|\\mathbf{z}) d\\mathbf{z} - \\nabla_{\\boldsymbol{\\phi}} KL(q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x}) | p(\\mathbf{z})), \\\\ \\end{aligned} $$\nThis is where the reparameterization trick comes in. Assuming $q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x})$ to be Gaussian, we reparameterize the encoder’s outputs:\n$$ \\begin{aligned} \\nabla_{\\boldsymbol{\\phi}} \\int \\textcolor{blue}{q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x})} \\log p_{\\boldsymbol{\\theta}}(\\mathbf{x}|\\textcolor{olive}{\\mathbf{z}}) d\\mathbf{z} \u0026= \\int \\textcolor{blue}{p(\\boldsymbol{\\epsilon})} \\nabla_{\\boldsymbol{\\phi}} \\log p_{\\boldsymbol{\\theta}}(\\mathbf{x}|\\textcolor{olive}{\\mathbf{g}_{\\boldsymbol{\\phi}}(\\mathbf{x}, \\boldsymbol{\\epsilon})}) d\\boldsymbol{\\epsilon} \\\\ \u0026\\approx \\nabla_{\\boldsymbol{\\phi}} \\log p_{\\boldsymbol{\\theta}}(\\mathbf{x}|\\textcolor{olive}{\\boldsymbol{\\sigma}_{\\boldsymbol{\\phi}}(\\mathbf{x})} \\odot \\textcolor{blue}{\\boldsymbol{\\epsilon}^*} + \\textcolor{olive}{\\boldsymbol{\\mu}_{\\boldsymbol{\\phi}}(\\mathbf{x})}), \\quad \\textcolor{blue}{\\boldsymbol{\\epsilon}^*} \\sim \\mathcal{N}(0, \\mathbf{I}), \\end{aligned} $$\nso we move the randomness from $\\mathbf{z} \\sim q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x})$ to $\\boldsymbol{\\epsilon} \\sim p(\\boldsymbol{\\epsilon})$ and use a deterministic transform $\\mathbf{z} = \\mathbf{g}_{\\boldsymbol{\\phi}}(\\mathbf{x}, \\boldsymbol{\\epsilon})$ to get an unbiased gradient. Moreover, the normal assumptions for $q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x})$ and $p(\\mathbf{z})$ allow us to compute $KL$ analytically and thus calculate the gradient $\\nabla_{\\boldsymbol{\\phi}}KL(q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x}) | p(\\mathbf{z}))$.\nThis example highlights the reparameterization trick, crucial for getting unbiased gradient estimates in continuous latent space VAEs. However, discrete representations $\\mathbf{z}$ are often more natural for modalities like texts or graphs, leading us to discrete VAE latents. Therefore:\nOur encoder should output a discrete distribution. We need an analogue of the reparameterization trick for discrete distributions. Our decoder should input discrete random variables. The classical solution for discrete variable reparameterization is Gumbel-Softmax (Jang et al. 2017) or Concrete distribution (Maddison et al. 2017). This is one distribution proposed in parallel by two research groups.\nGumbel Distribution $$ g \\sim \\mathrm{Gumbel}(0, 1) \\quad \\Leftrightarrow \\quad g = -\\log( - \\log u), \\quad u \\sim \\mathrm{Uniform}[0, 1] $$\nTheorem (Gumbel-Max Trick) Let $g_k \\sim \\mathrm{Gumbel}(0, 1)$ for $k = 1, \\ldots, K$. Then a discrete random variable\n$$ c = \\arg\\max_k [\\log \\pi_k + g_k] $$\nhas a categorical distribution $c \\sim \\mathrm{Categorical}(\\boldsymbol{\\pi})$.\nWe can sample from the discrete distribution using Gumbel-Max reparameterization. Parameters and random variable sampling are separated (reparameterization trick). Problem: The $\\arg\\max$ operation is non-differentiable. To overcome the above problem, the authors propose to consider one-hot Categorical distribution, that is simply replace random variable $c \\in \\{1, \\ldots, K\\}$ with a random vector $\\mathbf{c} \\in \\{0, 1\\}^{K}$, such that $$ c = k \\quad \\Leftrightarrow \\quad \\mathbf{c}_k = 1. $$\nGumbel-Softmax Relaxation $$ \\hat{\\mathbf{c}} = \\mathrm{softmax}\\left( \\frac{\\log \\boldsymbol{\\pi} + \\mathbf{g}}{\\tau} \\right) $$\nFurther, they relax $\\arg\\max$ operation into $\\mathrm{softmax}$, introducing a temperature parameter $\\tau \u003e 0$. As temperature $\\tau \\to \\infty$, Gumbel-Softmax distribution $\\mathrm{GS}(\\boldsymbol{\\pi}, \\tau)$ becomes more uniformly distributed (consider the analogy with melting ice). In contrast, as temperature limits to zero, that is $\\tau \\to 0$, the distribution becomes more and more similar with initial one — defined with $\\arg\\max$.\nFig. 2. Illustration of Gumbel-Softmax trick. The key idea is that $\\mathrm{softmax}$ is differential function, which allows us to do backward pass in neural networks! Such technique is called relaxation.\nOther relaxation methods We talked about one example of a relaxation for VAE with discrete latents — Gumbel-Softmax. There are other ways too. They give more flexible and better estimates of gradients. The rest of the blog-post is about these other relaxation methods.\nPackage Contents In this section, we shortly discuss each of the methods implemented in our Python library “Just Relax It”. Firstly, we generalize all relaxation methods into the following problem formulation (see Introduction for details):\nGiven discrete random variable $\\mathbf{c} \\sim p_{\\boldsymbol{\\phi}}(\\mathbf{c})$, estimate the gradient w.r.t. $\\boldsymbol{\\phi}$ of the expected value of some deterministic function $f(\\mathbf{c})$, using reparameterization trick with relaxation $\\mathbf{c} \\approx \\hat{\\mathbf{c}}(\\mathbf{z}, \\tau)$, where $\\mathbf{z} \\sim p(\\mathbf{z})$ and $\\tau \u003e 0$ is a temperature parameter. In other words,\n$$ \\nabla_{\\boldsymbol{\\phi}} \\mathbb{E}_{p_{\\boldsymbol{\\phi}}(\\mathbf{c})} f(\\mathbf{c}) \\approx \\mathbb{E}_{p(\\mathbf{z})} \\left[ \\nabla_{\\boldsymbol{\\phi}} f(\\hat{\\mathbf{c}}(\\mathbf{z}, \\tau)) \\right]. $$\nRelaxed Bernoulli (Yamada \u0026 Lindenbaum et al. 2018) This method relaxes Bernoulli random variable $c \\sim \\mathrm{Be}(\\pi)$. The idea is to clip a Gaussian random variable onto $(0, 1)$ and tune the mean $\\mu$ and scale $\\sigma$ instead of $\\pi$:\n$$ \\begin{aligned} \\epsilon \u0026\\sim \\mathcal{N}(0, 1),\\\\ z \u0026= \\sigma \\cdot \\epsilon + \\mu,\\\\ \\hat{c} \u0026= \\min (1, \\max (0, z)), \\end{aligned} $$\nwhere $\\mu$ is trainable and $\\sigma$ is fixed during training.\nHard Concrete (Louizos et al. 2018) This method relaxes Bernoulli random variable $c \\sim \\mathrm{Be}(\\pi)$. It is inspired by Relaxed Bernoulli, but uses another random variable to clip onto $(0, 1)$ — Gumbel-Softmax. Moreover, they additionally “stretch” it from $(0, 1)$ to the wider interval $(a, b)$, where $a \u003c 0$ and $b \u003e 0$ as follows:\n$$ \\begin{aligned} z \u0026\\sim \\mathrm{GS}(\\pi, \\tau),\\\\ \\tilde{z} \u0026= (b - a) \\cdot z + a,\\\\ \\hat{c} \u0026= \\min (1, \\max (0, \\tilde{z})), \\end{aligned} $$\nThis method applies hard-sigmoid technique to make two delta peaks at zero and one.\nStraight-Through Bernoulli (Cheng et al. 2019) This method relaxes Bernoulli random variable $c \\sim \\mathrm{Be}(\\pi)$. In order to achieve gradient flow through distribution parameter $\\pi$, this method replaces the gradient w.r.t. $c$ with the gradient of a continuous relaxation $\\pi$. Other words, it can be formulated using torch notation as follows:\n$$ \\hat{c} = \\pi + (c - \\pi)\\verb|.detach()|. $$\nIt means that on the forward pass $\\hat{c} = c$, but on the backward pass $\\hat{c} = \\pi$.\nStochastic Times Smooth (Bengio et al. 2013) This method relaxes Bernoulli random variable $c \\sim \\mathrm{Be}(\\pi)$ by the following:\n$$ \\begin{aligned} p \u0026= \\sigma(a), \\\\ b \u0026\\sim \\mathrm{Be}(\\sqrt{p}), \\\\ \\hat{c} \u0026= b \\sqrt{p}, \\end{aligned} $$\nwhere $a$ is a parameter of this distribution.\nCorrelated relaxed Bernoulli (Lee \u0026 Imrie et al. 2022) This method relaxes multivariate Bernoulli random variable $\\mathbf{c} \\sim \\mathrm{MultiBe}(\\boldsymbol{\\pi}, \\mathbf{R})$.\nMultivariate Bernoulli distribution assumes that each component $c_{k}$ can be correlated with others. It can be constructed via Gaussian copula $C_{\\mathbf{R}}$ (Nelsen, 2007):\n$$ C_{\\mathbf{R}}(u_1, \\ldots, u_p) = \\Phi_{\\mathbf{R}}(\\Phi^{-1}(u_1), \\ldots, \\Phi^{-1}(u_p)), $$\nwhere $\\Phi_{\\mathbf{R}}$ is the joint CDF of a multivariate Gaussian distribution with correlation matrix $\\mathbf{R}$, and $\\Phi^{-1}$ is the inverse CDF of the standard univariate Gaussian distribution. Each component is defined as follows:\n$$ c_{k} = \\begin{cases} 1, \\quad u_k \\leq \\pi_k,\\\\ 0, \\quad u_k \u003e \\pi_k. \\end{cases} $$\nRelaxation is applied as follows:\n$$ \\hat{\\mathbf{c}} = \\sigma \\left( \\frac{1}{\\tau} \\left( \\log \\frac{\\mathbf{u}}{1 - \\mathbf{u}} + \\log \\frac{\\boldsymbol{\\pi}}{1 - \\boldsymbol{\\pi}} \\right) \\right), $$\nwhere $\\sigma(x) = (1 + \\exp(-x))^{-1}$ is a sigmoid function, and $\\tau$ is a temperature hyperparameter.\nInvertible Gaussian (Potapczynski et al. 2019) This method relaxes Categorical random variable $\\mathbf{c} \\sim \\mathrm{Cat}(\\boldsymbol{\\pi})$. The idea is to transform Gaussian noise $\\boldsymbol{\\epsilon}$ through invertible transformation $g(\\cdot, \\tau)$ with temperature $\\tau$ onto the simplex:\n$$ \\begin{aligned} \\boldsymbol{\\epsilon} \u0026\\sim \\mathcal{N}(0, \\mathbf{I}),\\\\ \\mathbf{z} \u0026= \\boldsymbol{\\sigma} \\odot \\boldsymbol{\\epsilon} + \\boldsymbol{\\mu},\\\\ \\hat{\\mathbf{c}} \u0026= g(\\mathbf{z}, \\tau) = \\mathrm{softmax}_{++}(\\mathbf{z/\\tau}), \\end{aligned} $$\nwhere the latter function remains invertible via introduction $\\delta \u003e 0$ as follows:\n$$ \\mathrm{softmax}_{++}(\\mathbf{z/\\tau}) = \\frac{\\exp(\\mathbf{z}/\\tau)}{\\sum_{k=1}^{K} \\exp(z_k/\\tau) + \\delta}. $$\nGumbel-Softmax TOP-K (Kool et al. 2019) This method relaxes Categorical random variable $\\mathbf{c} \\sim \\mathrm{Cat}(\\boldsymbol{\\pi})$. However, it solves quite different problem. Suppose we want to get $K$ samples without replacement (i.e., not repeating) according to the Categorical distribution with probabilities $\\boldsymbol{\\pi}$. Similar to the Gumbel-Max method, let $g_k \\sim \\mathrm{Gumbel}(0, 1)$ for $k = 1, \\ldots, K$, then the Gumbel-Max TOP-K Theorem says, that the values of the form\n$$ c_1, \\ldots , c_K = \\mathrm{arg}\\underset{k}{\\mathrm{top}\\text{-}\\mathrm{K}} [ \\log\\pi_k + g_k] $$\nhave the $\\mathrm{Cat}(\\boldsymbol{\\pi})$ distribution without replacement.\nThis approach has all the same pros and cons as the classical Gumbel-Max trick, however, they can be fixed with the Gumbel-Softmax relaxation using a simple loop:\n$$ \\begin{aligned} \u0026\\text{for } k = 1, \\dots, K \\\\ \u0026\\quad 1. \\quad c_k \\sim \\mathrm{GS}(\\boldsymbol{\\pi}, \\tau) \\\\ \u0026\\quad 2. \\quad \\pi_k = -\\infty \\end{aligned} $$\nClosed-form Laplace Bridge (Hobbhahn et al. 2020) This method is not a relaxation technique, but is quite useful in terms of dicrete variables approximation. This is an approach of approximating Dirichlet distribution with Logistic-Normal, and vice versa.\nIn particular, the analytic map from the Dirichlet distribution parameter $\\boldsymbol{\\alpha} \\in \\mathbb{R}_{+}^{K}$ to the parameters of the Gaussian $\\boldsymbol{\\mu} \\in \\mathbb{R}^{K}$ and symmetric positive definite $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{K \\times K}$ is given by\n$$ \\begin{aligned} \\mu_i \u0026= \\log \\alpha_i - \\frac{1}{K} \\sum_{k=1}^{K} \\log \\alpha_k,\\\\ \\Sigma_{ij} \u0026= \\delta_{ij} \\frac{1}{\\alpha_i} - \\frac{1}{K} \\left( \\frac{1}{\\alpha_i} + \\frac{1}{\\alpha_j} - \\frac{1}{K} \\sum_{k=1}^{K} \\frac{1}{\\alpha_k} \\right), \\end{aligned} $$\nand the pseudo-inverse of this one, which maps the Gaussian parameters to those of the Dirichlet as\n$$ \\alpha_k = \\frac{1}{\\Sigma_{kk}} \\left( 1 - \\frac{2}{K} + \\frac{e^{\\mu_k}}{K^2} \\sum_{l=1}^{K} e^{-\\mu_l} \\right). $$\nAnd this is what is called Laplace Bridge between Dirichlet and Logistic-Normal distributions.\nImplementation (see our GitHub for details) In this section we describe our package design. The most famous Python probabilistic libraries with a built-in differentiation engine are PyTorch and Pyro. Thus, we implement the relaxit library consistently with both of them. Specifically, we\nTake a base class for PyTorch-compatible distributions with Pyro support TorchDistribution, for which we refer to this page on documentation. Inherent each of the considered relaxed distributions from this TorchDistribution. Implement batch_shape and event_shape properties that defines the distribution samples shapes. Implement rsample() and log_prob() methods as key two of the proposed algorithms. These methods are responsible for sample with reparameterization trick and log-likelihood computing respectively. For closed-form Laplace Bridge between Dirichlet and Logistic-Normal distributions we extend the base PyTorch KL-divergence method with one more realization. We also implement a LogisticNormalSoftmax distribution, which is a transformed distribution from the Normal one. In contrast to original LogisticNormal from Pyro or PyTorch, this one uses SoftmaxTransform, instead of StickBreakingTransform that allows us to remain in the same dimensionality.\nDemo Our demo code is available at this link. For demonstration purposes, we divide our algorithms in two different groups. Each group relates to the particular experiment:\nLaplace Bridge between Dirichlet and Logistic-Normal distributions; Other relaxation methods. Laplace Bridge. This part relates to the demonstration of closed-form Laplace Bridge between Dirichlet and Logistic-Normal distributions. We subsequently 1) initialize a Dirichlet distribution with random parameters; 2) approximate it with a Logistic-Normal distribution; 3) approximate obtained Logistic-Normal distribution with Dirichlet one.\nDirichlet (with random parameters) Logistic-Normal (approximation to Dirichlet) Dirichlet (approximation to obtained Logistic-Normal) VAE with discrete latents. All the other 7 methods are used to train a VAE with discrete latents. Each of the discussed relaxation techniques allows us to learn the latent space with the corresponding distribution. All implemented distributions have a similar structure, so we chose one distribution for demonstration and conducted a number of experiments with it — Correlated Relaxed Bernoulli. This method generates correlated gate vectors from a multivariate Bernoulli distribution using a Gaussian copula. We define the parameters $\\boldsymbol{\\pi}$, $\\mathbf{R}$, and $\\tau$ as follows:\nTensor $\\boldsymbol{\\pi}$, representing the probabilities of the Bernoulli distribution, with an event shape of 3 and a batch size of 2: $$ \\boldsymbol{\\pi} = \\begin{bmatrix} 0.2 \u0026 0.4 \u0026 0.4 \\\\ 0.3 \u0026 0.5 \u0026 0.2 \\end{bmatrix} $$\nCorrelation matrix $\\mathbf{R}$ for the Gaussian copula: $$ \\mathbf{R} = \\begin{bmatrix} 1.0 \u0026 0.5 \u0026 0.3 \\\\ 0.5 \u0026 1.0 \u0026 0.7 \\\\ 0.3 \u0026 0.7 \u0026 1.0 \\end{bmatrix} $$\nTemperature hyperparameter $\\tau = 0.1$ Finally, after training we obtained reconstruction and sampling results for a MNIST dataset that we provide below. We see that VAE has learned something adequate, which means that the reparameterization is happening correctly. For the rest of the methods, VAE are also implemented, which you can get engaged using scripts in the demo experiments directory.\nFig. 3. VAE with Correlated Relaxed Bernoulli latents. Reconstruction. Fig. 4. VAE with Correlated Relaxed Bernoulli latents. Sampling. Conclusion In summary, Just Relax It is a powerful tool for researchers and practitioners working with discrete variables in neural networks. By offering a comprehensive set of relaxation techniques, our library aims to make the optimization process more efficient and accessible. We encourage you to explore our library, try out the demo, and contribute to its development. Together, we can push the boundaries of what is possible with discrete variable relaxation in machine learning.\nThank you for reading, and happy coding!\nDaniil Dorin, Igor Ignashin, Nikita Kiselev, Andrey Veprikov\nReferences [1] Kingma and Welling. “Auto-Encoding Variational Bayes”. arXiv preprint arXiv:1312.6114 (2013).\n[2] Jang et al. “Categorical Reparameterization with Gumbel-Softmax”. arXiv preprint arXiv:1611.01144 (2016).\n[3] Maddison et al. “The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables”. arXiv preprint arXiv:1611.00712 (2016).\n[4] Yamada, Lindenbaum et al. “Feature Selection using Stochastic Gates”. PMLR, 2020.\n[5] Lee, Imrie et al. “Self-Supervision Enhanced Feature Selection with Correlated Gates”. ICLR, 2022.\n[6] Kool et al. “Stochastic Beams and Where to Find Them: The Gumbel-Top-k Trick for Sampling Sequences Without Replacement”. PMLR, 2019.\n[7] Bengio et al. “Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation”. arXiv preprint arXiv:1308.3432 (2013).\n[8] Potapczynski et al. “Invertible Gaussian Reparameterization: Revisiting the Gumbel-Softmax”. arXiv preprint arXiv:1912.09588 (2019).\n[9] Louizos et al. “Learning Sparse Neural Networks through $L_0$ Regularization”. arXiv preprint arXiv:1712.01312 (2017).\n[10] Hobbhahn et al. “Fast Predictive Uncertainty for Classification with Bayesian Deep Networks”. PMLR, 2022.\n",
  "wordCount" : "2552",
  "inLanguage": "en",
  "image":"http://localhost:1313/overview.png","datePublished": "2024-12-07T00:00:00Z",
  "dateModified": "2024-12-07T00:00:00Z",
  "author":[{
    "@type": "Person",
    "name": "Daniil Dorin"
  }, {
    "@type": "Person",
    "name": "Igor Ignashin"
  }, {
    "@type": "Person",
    "name": "Nikita Kiselev"
  }, {
    "@type": "Person",
    "name": "Andrey Veprikov"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/projects/relaxit/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Nikita Kiselev",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js" integrity="sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"
  onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          delimiters: [
            {left: '$$', right: '$$', display: true},
            {left: '$', right: '$', display: false},
            {left: "\\begin{equation}", right: "\\end{equation}", display: true},
            {left: "\\begin{equation*}", right: "\\end{equation*}", display: true},
            {left: "\\begin{align}", right: "\\end{align}", display: true},
            {left: "\\begin{align*}", right: "\\end{align*}", display: true},
            {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
            {left: "\\begin{gather}", right: "\\end{gather}", display: true},
            {left: "\\begin{CD}", right: "\\end{CD}", display: true},
          ],
          throwOnError : false
        });
    });
</script>
 


</head>

<body class="" id="top">

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Nikita Kiselev">
                <img src="http://localhost:1313/favicon.ico" alt="" aria-label="logo"
                    height="18"
                    width="18">Nikita Kiselev</a>
            <div class="logo-switches">
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/publications/" title="Publications">
                    <span>Publications</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/talks/" title="Talks">
                    <span>Talks</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/projects/" title="Projects">
                    <span>Projects</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/resume.pdf" title="Resume">
                    <span>Resume</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/cv.pdf" title="CV">
                    <span>CV</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/archive/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
        </ul>
    </nav>
</header>

    <main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Just Relax It! Leveraging relaxation for discrete variables optimization
    </h1>
    <div class="post-meta"><span title='2024-12-07 00:00:00 +0000 UTC'>December 2024</span>&nbsp;&middot;&nbsp;12 min&nbsp;&middot;&nbsp;Daniil Dorin,&thinsp;Igor Ignashin,&thinsp;Nikita Kiselev,&thinsp;Andrey Veprikov&nbsp;&middot;&nbsp;<a href="https://github.com/intsystems/relaxit" rel="noopener noreferrer" target="_blank">GitHub</a>

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><nav id="TableOfContents">
  <ul>
    <li><a href="#introduction-a-nameintroductiona">Introduction <a name="introduction"></a></a></li>
    <li><a href="#package-contents">Package Contents</a></li>
    <li><a href="#implementation-see-our-githubhttpsgithubcomintsystemsrelaxit-for-details">Implementation (see our <a href="https://github.com/intsystems/relaxit">GitHub</a> for details)</a></li>
    <li><a href="#demo">Demo</a></li>
    <li><a href="#conclusion">Conclusion</a></li>
    <li><a href="#references">References</a></li>
  </ul>
</nav>
        </div>
    </details>
</div>

  <div class="post-content"><img src="overview.png" style="width: 80%;" class="center" />
<p>In this blog-post we present our Python library <a href="https://github.com/intsystems/relaxit" target="_blank">&ldquo;Just Relax It&rdquo;</a> (or <code>relaxit</code>) designed to streamline the optimization of discrete probability distributions in neural networks, offering a suite of advanced relaxation techniques compatible with PyTorch.</p>
<h2 id="introduction-a-nameintroductiona">Introduction <a name="introduction"></a></h2>
<p>Rapid advancement of generative models, such as Variational Autoencoders (VAEs) and Diffusion Models, has driven the development of relevant mathematical tools.
Any generative model contains some source of randomness to make new objects.
This randomness is represented by a probability distribution, from which random variables are sampled.
Therefore, training a generative model often boils down to optimizing the parameters of this distribution.</p>
<p>Pioneering generative models typically work with <strong>continuous</strong> distributions like the Normal distribution.
However, for some modalities, such as texts or graphs, it is more natural to use <strong>discrete</strong> distributions — Bernoulli, Categorical, etc.</p>
<p>Thus, we present our new Python library <a href="https://github.com/intsystems/relaxit" target="_blank">&ldquo;Just Relax It&rdquo;</a> that combines the best techniques for relaxing discrete distributions (we will explain what that means later) into an easy-to-use package. And it is compatible with PyTorch!</p>
<p>We start with a basic Variational Autoencoder (VAE) example that shows how parameter optimization typically happens for continuous distributions, then we move on to the case of discrete distributions.
After that, we overview relaxation methods used in our library and provide a demo of training a VAE with discrete latent variables.</p>


<style>
    summary h3 {
        display: inline;
        margin: 0;
    }
</style>
<p>
    <details >
        <summary markdown="span">
            <h3>VAE Example</h3>
        </summary>
        <p>We assume that you are already familiar with the concept of a Variational Autoencoder (VAE), at least at the level that it consists of two parts: an encoder and a decoder, and tries to approximate the distribution $p(\mathbf{x})$ of data $\mathbf{x}$ using latent variables $\mathbf{z}$. If this is not the case, then we recommend that you to read a <a href="https://lilianweng.github.io/posts/2018-08-12-vae/" target="_blank">blog-post</a> by Lilian Weng.</p>
<img src="demo.png" style="width: 80%;" class="center" />
<figcaption>Fig. 1. Illustration of Variational Autoencoder (VAE) architecture. </figcaption>
<p>The original VAE (<a href="https://arxiv.org/abs/1312.6114" target="_blank">Kingma &amp; Welling, 2014</a>) has two main parts:</p>
<ol>
<li><strong>Encoder</strong> $q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x})$: A neural network $g_{\boldsymbol{\phi}}(\mathbf{x})$ that outputs parameters of the latent Gaussian distribution.</li>
<li><strong>Decoder</strong> $p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z})$: A neural network $f_{\boldsymbol{\theta}}(\mathbf{z})$ that outputs parameters of the sample distribution (usually Gaussian or Bernoulli).</li>
</ol>
<p>Training a VAE involves maximizing the ELBO (evidence lower bound) with respect to the encoder and decoder parameters:</p>
<p>$$
\mathcal{L}_{\boldsymbol{\phi}, \boldsymbol{\theta}}(\mathbf{x}) = \mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x})} \log p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z}) - KL(q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) | p(\mathbf{z})) \to \max_{\boldsymbol{\phi}, \boldsymbol{\theta}}.
$$</p>
<p>During the <strong>M-step</strong>, we derive an unbiased estimator for the gradient $\nabla_{\boldsymbol{\theta}}\mathcal{L}_{\boldsymbol{\phi}, \boldsymbol{\theta}}(\mathbf{x})$:</p>
<p>$$
\begin{aligned}
\nabla_{\boldsymbol{\theta}}\mathcal{L}_{\boldsymbol{\phi}, \boldsymbol{\theta}}(\mathbf{x})
&amp;= \textcolor{blue}{\nabla_{\boldsymbol{\theta}}} \int q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) \log p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z}) d\mathbf{z} \\
&amp;= \int q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) \textcolor{blue}{\nabla_{\boldsymbol{\theta}}} \log p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z}) d\mathbf{z} \\
&amp;\approx \nabla_{\boldsymbol{\theta}} \log p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z}^*), \quad \mathbf{z}^* \sim q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}),
\end{aligned}
$$</p>
<p>where the last approximation uses Monte-Carlo sampling.</p>
<p>However, during the <strong>E-step</strong>, getting an unbiased estimator for the gradient $\nabla_{\boldsymbol{\phi}}\mathcal{L}_{\boldsymbol{\phi}, \boldsymbol{\theta}}(\mathbf{x})$ is tricky because the density function $q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x})$ depends on $\boldsymbol{\phi}$. This makes Monte-Carlo estimation impossible:</p>
<p>$$
\begin{aligned}
\nabla_{\boldsymbol{\phi}}\mathcal{L}_{\boldsymbol{\phi}, \boldsymbol{\theta}}(\mathbf{x})
&amp;= \textcolor{blue}{\nabla_{\boldsymbol{\phi}}} \int q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) \log p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z}) d\mathbf{z} - \nabla_{\boldsymbol{\phi}} KL(q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) | p(\mathbf{z})) \\
&amp;\textcolor{red}{\neq} \int q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) \textcolor{blue}{\nabla_{\boldsymbol{\phi}}} \log p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z}) d\mathbf{z} - \nabla_{\boldsymbol{\phi}} KL(q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) | p(\mathbf{z})), \\
\end{aligned}
$$</p>
<p>This is where the <strong>reparameterization trick</strong> comes in. Assuming $q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x})$ to be Gaussian, we reparameterize the encoder&rsquo;s outputs:</p>
<p>$$
\begin{aligned}
\nabla_{\boldsymbol{\phi}} \int \textcolor{blue}{q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x})} \log p_{\boldsymbol{\theta}}(\mathbf{x}|\textcolor{olive}{\mathbf{z}}) d\mathbf{z}
&amp;= \int \textcolor{blue}{p(\boldsymbol{\epsilon})} \nabla_{\boldsymbol{\phi}} \log p_{\boldsymbol{\theta}}(\mathbf{x}|\textcolor{olive}{\mathbf{g}_{\boldsymbol{\phi}}(\mathbf{x}, \boldsymbol{\epsilon})}) d\boldsymbol{\epsilon} \\
&amp;\approx \nabla_{\boldsymbol{\phi}} \log p_{\boldsymbol{\theta}}(\mathbf{x}|\textcolor{olive}{\boldsymbol{\sigma}_{\boldsymbol{\phi}}(\mathbf{x})} \odot \textcolor{blue}{\boldsymbol{\epsilon}^*} + \textcolor{olive}{\boldsymbol{\mu}_{\boldsymbol{\phi}}(\mathbf{x})}), \quad \textcolor{blue}{\boldsymbol{\epsilon}^*} \sim \mathcal{N}(0, \mathbf{I}),
\end{aligned}
$$</p>
<p>so we move the randomness from $\mathbf{z} \sim q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x})$ to $\boldsymbol{\epsilon} \sim p(\boldsymbol{\epsilon})$ and use a deterministic transform $\mathbf{z} = \mathbf{g}_{\boldsymbol{\phi}}(\mathbf{x}, \boldsymbol{\epsilon})$ to get an unbiased gradient. Moreover, the normal assumptions for $q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x})$ and $p(\mathbf{z})$ allow us to compute $KL$ analytically and thus calculate the gradient $\nabla_{\boldsymbol{\phi}}KL(q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) | p(\mathbf{z}))$.</p>
<p>This example highlights the reparameterization trick, crucial for getting unbiased gradient estimates in continuous latent space VAEs. However, discrete representations $\mathbf{z}$ are often more natural for modalities like texts or graphs, leading us to <strong>discrete VAE latents</strong>. Therefore:</p>
<ul>
<li>Our encoder should output a discrete distribution.</li>
<li>We need an analogue of the reparameterization trick for discrete distributions.</li>
<li>Our decoder should input discrete random variables.</li>
</ul>
<p>The classical solution for discrete variable reparameterization is <strong>Gumbel-Softmax</strong> (<a href="https://arxiv.org/abs/1611.01144" target="_blank">Jang et al. 2017</a>) or <strong>Concrete distribution</strong> (<a href="https://arxiv.org/abs/1611.00712" target="_blank">Maddison et al. 2017</a>). This is one distribution proposed in parallel by two research groups.</p>
<h4 id="gumbel-distribution">Gumbel Distribution</h4>
<p>$$ g \sim \mathrm{Gumbel}(0, 1) \quad \Leftrightarrow \quad g = -\log( - \log u), \quad u \sim \mathrm{Uniform}[0, 1] $$</p>
<h4 id="theorem-gumbel-max-trick">Theorem (Gumbel-Max Trick)</h4>
<p>Let $g_k \sim \mathrm{Gumbel}(0, 1)$ for $k = 1, \ldots, K$. Then a discrete random variable</p>
<p>$$ c = \arg\max_k [\log \pi_k + g_k] $$</p>
<p>has a categorical distribution $c \sim \mathrm{Categorical}(\boldsymbol{\pi})$.</p>
<ul>
<li>We can sample from the discrete distribution using Gumbel-Max reparameterization.</li>
<li>Parameters and random variable sampling are separated (reparameterization trick).</li>
<li><strong>Problem:</strong> The $\arg\max$ operation is non-differentiable.</li>
</ul>
<p>To overcome the above problem, the authors propose to consider <strong>one-hot Categorical distribution</strong>, that is simply replace random variable $c \in \{1, \ldots, K\}$ with a random vector $\mathbf{c} \in \{0, 1\}^{K}$, such that
$$ c = k \quad \Leftrightarrow \quad \mathbf{c}_k = 1. $$</p>
<h4 id="gumbel-softmax-relaxation">Gumbel-Softmax Relaxation</h4>
<p>$$ \hat{\mathbf{c}} = \mathrm{softmax}\left( \frac{\log \boldsymbol{\pi} + \mathbf{g}}{\tau} \right) $$</p>
<p>Further, they <strong>relax</strong> $\arg\max$ operation into $\mathrm{softmax}$, introducing a temperature parameter $\tau &gt; 0$.
As temperature $\tau \to \infty$, Gumbel-Softmax distribution $\mathrm{GS}(\boldsymbol{\pi}, \tau)$ becomes more uniformly distributed (consider the analogy with melting ice).
In contrast, as temperature limits to zero, that is $\tau \to 0$, the distribution becomes more and more similar with initial one — defined with $\arg\max$.</p>
<img src="gumbel-softmax.png" style="width: 60%;" class="center" />
<figcaption>Fig. 2. Illustration of Gumbel-Softmax trick. </figcaption>
<p>The <strong>key idea</strong> is that $\mathrm{softmax}$ is differential function, which allows us to do backward pass in neural networks!
Such technique is called <strong>relaxation</strong>.</p>

    </details>
</p>
<h3 id="other-relaxation-methods">Other relaxation methods</h3>
<p>We talked about one example of a relaxation for VAE with discrete latents — Gumbel-Softmax.
There are other ways too.
They give more flexible and better estimates of gradients.
The rest of the blog-post is about these other relaxation methods.</p>
<h2 id="package-contents">Package Contents</h2>
<p>In this section, we shortly discuss each of the methods implemented in our Python library <a href="https://github.com/intsystems/relaxit" target="_blank">&ldquo;Just Relax It&rdquo;</a>.
Firstly, we generalize all relaxation methods into the following problem formulation (see <a href="#introduction">Introduction</a> for details):</p>
<blockquote>
<p>Given discrete random variable $\mathbf{c} \sim p_{\boldsymbol{\phi}}(\mathbf{c})$, estimate the gradient w.r.t. $\boldsymbol{\phi}$ of the expected value of some deterministic function $f(\mathbf{c})$, using reparameterization trick with relaxation $\mathbf{c} \approx \hat{\mathbf{c}}(\mathbf{z}, \tau)$, where $\mathbf{z} \sim p(\mathbf{z})$ and $\tau &gt; 0$ is a temperature parameter. In other words,</p>
</blockquote>
<p>$$
\nabla_{\boldsymbol{\phi}} \mathbb{E}_{p_{\boldsymbol{\phi}}(\mathbf{c})} f(\mathbf{c}) \approx \mathbb{E}_{p(\mathbf{z})} \left[ \nabla_{\boldsymbol{\phi}} f(\hat{\mathbf{c}}(\mathbf{z}, \tau)) \right].
$$</p>


<style>
    summary h3 {
        display: inline;
        margin: 0;
    }
</style>
<p>
    <details >
        <summary markdown="span">
            <h3>Relaxed Bernoulli (<a href="https://arxiv.org/abs/1810.04247" target="_blank">Yamada &amp; Lindenbaum et al. 2018</a>)</h3>
        </summary>
        <p><a name="relaxedbernoulli"></a></p>
<p>This method relaxes <strong>Bernoulli</strong> random variable $c \sim \mathrm{Be}(\pi)$.
The idea is to clip a Gaussian random variable onto $(0, 1)$ and tune the mean $\mu$ and scale $\sigma$ instead of $\pi$:</p>
<p>$$
\begin{aligned}
\epsilon &amp;\sim \mathcal{N}(0, 1),\\
z &amp;= \sigma \cdot \epsilon + \mu,\\
\hat{c} &amp;= \min (1, \max (0, z)),
\end{aligned}
$$</p>
<p>where $\mu$ is trainable and $\sigma$ is fixed during training.</p>

    </details>
</p>


<style>
    summary h3 {
        display: inline;
        margin: 0;
    }
</style>
<p>
    <details >
        <summary markdown="span">
            <h3>Hard Concrete (<a href="https://arxiv.org/abs/1712.01312" target="_blank">Louizos et al. 2018</a>)</h3>
        </summary>
        <p><a></a></p>
<p>This method relaxes <strong>Bernoulli</strong> random variable $c \sim \mathrm{Be}(\pi)$.
It is inspired by <a href="#relaxedbernoulli">Relaxed Bernoulli</a>, but uses another random variable to clip onto $(0, 1)$ — Gumbel-Softmax.
Moreover, they additionally &ldquo;stretch&rdquo; it from $(0, 1)$ to the wider interval $(a, b)$, where $a &lt; 0$ and $b &gt; 0$ as follows:</p>
<p>$$
\begin{aligned}
z &amp;\sim \mathrm{GS}(\pi, \tau),\\
\tilde{z} &amp;= (b - a) \cdot z + a,\\
\hat{c} &amp;= \min (1, \max (0, \tilde{z})),
\end{aligned}
$$</p>
<p>This method applies hard-sigmoid technique to make two delta peaks at zero and one.</p>

    </details>
</p>


<style>
    summary h3 {
        display: inline;
        margin: 0;
    }
</style>
<p>
    <details >
        <summary markdown="span">
            <h3>Straight-Through Bernoulli (<a href="https://arxiv.org/abs/1910.02176" target="_blank">Cheng et al. 2019</a>)</h3>
        </summary>
        <p><a></a></p>
<p>This method relaxes <strong>Bernoulli</strong> random variable $c \sim \mathrm{Be}(\pi)$.
In order to achieve gradient flow through distribution parameter $\pi$, this method replaces the gradient w.r.t. $c$ with the gradient of a continuous relaxation $\pi$.
Other words, it can be formulated using <code>torch</code> <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.detach.html" target="_blank">notation</a> as follows:</p>
<p>$$
\hat{c} = \pi + (c - \pi)\verb|.detach()|.
$$</p>
<p>It means that on the <strong>forward pass</strong> $\hat{c} = c$, but on the <strong>backward pass</strong> $\hat{c} = \pi$.</p>

    </details>
</p>


<style>
    summary h3 {
        display: inline;
        margin: 0;
    }
</style>
<p>
    <details >
        <summary markdown="span">
            <h3>Stochastic Times Smooth (<a href="https://citeseerx.ist.psu.edu/document?repid=rep1&amp;type=pdf&amp;doi=62c76ca0b2790c34e85ba1cce09d47be317c7235" target="_blank">Bengio et al. 2013</a>)</h3>
        </summary>
        <p><a></a></p>
<p>This method relaxes <strong>Bernoulli</strong> random variable $c \sim \mathrm{Be}(\pi)$ by the following:</p>
<p>$$
\begin{aligned}
p &amp;= \sigma(a), \\
b &amp;\sim \mathrm{Be}(\sqrt{p}), \\
\hat{c} &amp;= b \sqrt{p},
\end{aligned}
$$</p>
<p>where $a$ is a parameter of this distribution.</p>

    </details>
</p>


<style>
    summary h3 {
        display: inline;
        margin: 0;
    }
</style>
<p>
    <details >
        <summary markdown="span">
            <h3>Correlated relaxed Bernoulli (<a href="https://openreview.net/pdf?id=oDFvtxzPOx" target="_blank">Lee &amp; Imrie et al. 2022</a>)</h3>
        </summary>
        <p><a></a></p>
<p>This method relaxes <strong>multivariate Bernoulli</strong> random variable $\mathbf{c} \sim \mathrm{MultiBe}(\boldsymbol{\pi}, \mathbf{R})$.</p>
<blockquote>
<p>Multivariate Bernoulli distribution assumes that each component $c_{k}$ can be correlated with others.
It can be constructed via Gaussian copula $C_{\mathbf{R}}$ (<a href="https://link.springer.com/book/10.1007/0-387-28678-0" target="_blank">Nelsen, 2007</a>):</p>
</blockquote>
<p>$$
C_{\mathbf{R}}(u_1, \ldots, u_p) = \Phi_{\mathbf{R}}(\Phi^{-1}(u_1), \ldots, \Phi^{-1}(u_p)),
$$</p>
<blockquote>
<p>where $\Phi_{\mathbf{R}}$ is the joint CDF of a multivariate Gaussian distribution with correlation matrix $\mathbf{R}$, and $\Phi^{-1}$ is the inverse CDF of the standard univariate Gaussian distribution. Each component is defined as follows:</p>
</blockquote>
<p>$$
c_{k} = \begin{cases} 1, \quad u_k \leq \pi_k,\\ 0, \quad u_k &gt; \pi_k. \end{cases}
$$</p>
<p>Relaxation is applied as follows:</p>
<p>$$
\hat{\mathbf{c}} = \sigma \left( \frac{1}{\tau} \left( \log \frac{\mathbf{u}}{1 - \mathbf{u}} + \log \frac{\boldsymbol{\pi}}{1 - \boldsymbol{\pi}} \right) \right),
$$</p>
<p>where $\sigma(x) = (1 + \exp(-x))^{-1}$ is a sigmoid function, and $\tau$ is a temperature hyperparameter.</p>

    </details>
</p>


<style>
    summary h3 {
        display: inline;
        margin: 0;
    }
</style>
<p>
    <details >
        <summary markdown="span">
            <h3>Invertible Gaussian (<a href="https://arxiv.org/abs/1912.09588" target="_blank">Potapczynski et al. 2019</a>)</h3>
        </summary>
        <p><a></a></p>
<p>This method relaxes <strong>Categorical</strong> random variable $\mathbf{c} \sim \mathrm{Cat}(\boldsymbol{\pi})$.
The idea is to transform Gaussian noise $\boldsymbol{\epsilon}$ through invertible transformation $g(\cdot, \tau)$ with temperature $\tau$ onto the simplex:</p>
<p>$$
\begin{aligned}
\boldsymbol{\epsilon} &amp;\sim \mathcal{N}(0, \mathbf{I}),\\
\mathbf{z} &amp;= \boldsymbol{\sigma} \odot \boldsymbol{\epsilon} + \boldsymbol{\mu},\\
\hat{\mathbf{c}} &amp;= g(\mathbf{z}, \tau) = \mathrm{softmax}_{++}(\mathbf{z/\tau}),
\end{aligned}
$$</p>
<p>where the latter function remains invertible via introduction $\delta &gt; 0$ as follows:</p>
<p>$$
\mathrm{softmax}_{++}(\mathbf{z/\tau}) = \frac{\exp(\mathbf{z}/\tau)}{\sum_{k=1}^{K} \exp(z_k/\tau) + \delta}.
$$</p>

    </details>
</p>


<style>
    summary h3 {
        display: inline;
        margin: 0;
    }
</style>
<p>
    <details >
        <summary markdown="span">
            <h3>Gumbel-Softmax TOP-K (<a href="https://arxiv.org/abs/1903.06059" target="_blank">Kool et al. 2019</a>)</h3>
        </summary>
        <p><a></a></p>
<p>This method relaxes <strong>Categorical</strong> random variable $\mathbf{c} \sim \mathrm{Cat}(\boldsymbol{\pi})$.
However, it solves quite different problem.
Suppose we want to get $K$ samples without replacement (i.e., not repeating) according to the Categorical distribution with probabilities $\boldsymbol{\pi}$. Similar to the Gumbel-Max method, let $g_k \sim \mathrm{Gumbel}(0, 1)$ for $k = 1, \ldots, K$, then the Gumbel-Max TOP-K Theorem says, that the values of the form</p>
<p>$$
c_1, \ldots , c_K = \mathrm{arg}\underset{k}{\mathrm{top}\text{-}\mathrm{K}} [ \log\pi_k + g_k]
$$</p>
<p>have the $\mathrm{Cat}(\boldsymbol{\pi})$ distribution without replacement.</p>
<p>This approach has all the same pros and cons as the classical Gumbel-Max trick, however, they can be fixed with the Gumbel-Softmax relaxation using a simple loop:</p>
<p>$$
\begin{aligned}
&amp;\text{for } k = 1, \dots, K \\
&amp;\quad 1. \quad c_k \sim \mathrm{GS}(\boldsymbol{\pi}, \tau) \\
&amp;\quad 2. \quad \pi_k = -\infty
\end{aligned}
$$</p>

    </details>
</p>


<style>
    summary h3 {
        display: inline;
        margin: 0;
    }
</style>
<p>
    <details >
        <summary markdown="span">
            <h3>Closed-form Laplace Bridge (<a href="https://arxiv.org/abs/2003.01227" target="_blank">Hobbhahn et al. 2020</a>)</h3>
        </summary>
        <p><a></a></p>
<p>This method is not a relaxation technique, but is quite useful in terms of dicrete variables approximation.
This is an approach of approximating Dirichlet distribution with Logistic-Normal, and vice versa.</p>
<p>In particular, the analytic map from the Dirichlet distribution parameter $\boldsymbol{\alpha} \in \mathbb{R}_{+}^{K}$ to the parameters of the Gaussian $\boldsymbol{\mu} \in \mathbb{R}^{K}$ and symmetric positive definite $\boldsymbol{\Sigma} \in \mathbb{R}^{K \times K}$ is given by</p>
<p>$$
\begin{aligned}
\mu_i &amp;= \log \alpha_i - \frac{1}{K} \sum_{k=1}^{K} \log \alpha_k,\\
\Sigma_{ij} &amp;= \delta_{ij} \frac{1}{\alpha_i} - \frac{1}{K} \left( \frac{1}{\alpha_i} + \frac{1}{\alpha_j} - \frac{1}{K} \sum_{k=1}^{K} \frac{1}{\alpha_k} \right),
\end{aligned}
$$</p>
<p>and the pseudo-inverse of this one, which maps the Gaussian parameters to those of the Dirichlet as</p>
<p>$$
\alpha_k = \frac{1}{\Sigma_{kk}} \left( 1 - \frac{2}{K} + \frac{e^{\mu_k}}{K^2} \sum_{l=1}^{K} e^{-\mu_l} \right).
$$</p>
<p>And this is what is called <strong>Laplace Bridge between Dirichlet and Logistic-Normal distributions</strong>.</p>

    </details>
</p>
<h2 id="implementation-see-our-githubhttpsgithubcomintsystemsrelaxit-for-details">Implementation (see our <a href="https://github.com/intsystems/relaxit" target="_blank">GitHub</a> for details)</h2>
<p>In this section we describe our package design. The most famous Python probabilistic libraries with a built-in differentiation engine are <a href="https://pytorch.org/docs/stable/index.html" target="_blank">PyTorch</a> and <a href="https://docs.pyro.ai/en/dev/index.html" target="_blank">Pyro</a>. Thus, we implement the <code>relaxit</code> library consistently with both of them. Specifically, we</p>
<ol>
<li>Take a base class for PyTorch-compatible distributions with Pyro support <code>TorchDistribution</code>, for which we refer to <a href="https://docs.pyro.ai/en/dev/distributions.html#torchdistribution" target="_blank">this page</a> on documentation.</li>
<li>Inherent each of the considered relaxed distributions from this <code>TorchDistribution</code>.</li>
<li>Implement <code>batch_shape</code> and <code>event_shape</code> properties that defines the distribution samples shapes.</li>
<li>Implement <code>rsample()</code> and <code>log_prob()</code> methods as key two of the proposed algorithms. These methods are responsible for sample with reparameterization trick and log-likelihood computing respectively.</li>
</ol>
<p>For closed-form Laplace Bridge between Dirichlet and Logistic-Normal distributions we extend the base PyTorch KL-divergence method with one more realization. We also implement a <code>LogisticNormalSoftmax</code> distribution, which is a transformed distribution from the <code>Normal</code> one. In contrast to original <code>LogisticNormal</code> from Pyro or PyTorch, this one uses <code>SoftmaxTransform</code>, instead of <code>StickBreakingTransform</code> that allows us to remain in the same dimensionality.</p>
<h2 id="demo">Demo</h2>
<p>Our demo code is available at <a href="https://github.com/intsystems/relaxit/tree/main/demo" target="_blank">this link</a>. For demonstration purposes, we divide our algorithms in two different groups. Each group relates to the particular experiment:</p>
<ol>
<li>Laplace Bridge between Dirichlet and Logistic-Normal distributions;</li>
<li>Other relaxation methods.</li>
</ol>
<p><strong>Laplace Bridge.</strong> This part relates to the demonstration of closed-form Laplace Bridge between Dirichlet and Logistic-Normal distributions. We subsequently 1) initialize a Dirichlet distribution with random parameters; 2) approximate it with a Logistic-Normal distribution; 3) approximate obtained Logistic-Normal distribution with Dirichlet one.</p>
<table>
<thead>
<tr>
<th style="text-align:center">Dirichlet <br> (with random parameters)</th>
<th style="text-align:center">Logistic-Normal <br> (approximation to Dirichlet)</th>
<th style="text-align:center">Dirichlet <br> (approximation to obtained Logistic-Normal)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img loading="lazy" src="laplace-bridge-1.png" alt=""  />
</td>
<td style="text-align:center"><img loading="lazy" src="laplace-bridge-2.png" alt=""  />
</td>
<td style="text-align:center"><img loading="lazy" src="laplace-bridge-3.png" alt=""  />
</td>
</tr>
</tbody>
</table>
<p><strong>VAE with discrete latents.</strong> All the other 7 methods are used to train a VAE with discrete latents. Each of the discussed relaxation techniques allows us to learn the latent space with the corresponding distribution. All implemented distributions have a similar structure, so we chose one distribution for demonstration and conducted a number of experiments with it — <strong>Correlated Relaxed Bernoulli</strong>. This method generates correlated gate vectors from a multivariate Bernoulli distribution using a Gaussian copula. We define the parameters $\boldsymbol{\pi}$, $\mathbf{R}$, and $\tau$ as follows:</p>
<ul>
<li>Tensor $\boldsymbol{\pi}$, representing the probabilities of the Bernoulli distribution, with an event shape of 3 and a batch size of 2:</li>
</ul>
<p>$$
\boldsymbol{\pi} = \begin{bmatrix}
0.2 &amp; 0.4 &amp; 0.4 \\
0.3 &amp; 0.5 &amp; 0.2
\end{bmatrix}
$$</p>
<ul>
<li>Correlation matrix $\mathbf{R}$ for the Gaussian copula:</li>
</ul>
<p>$$
\mathbf{R} = \begin{bmatrix}
1.0 &amp; 0.5 &amp; 0.3 \\
0.5 &amp; 1.0 &amp; 0.7 \\
0.3 &amp; 0.7 &amp; 1.0
\end{bmatrix}
$$</p>
<ul>
<li>Temperature hyperparameter $\tau = 0.1$</li>
</ul>
<p>Finally, after training we obtained reconstruction and sampling results for a MNIST dataset that we provide below. We see that VAE has learned something adequate, which means that the reparameterization is happening correctly. For the rest of the methods, VAE are also implemented, which you can get engaged using scripts in the demo experiments directory.</p>
<img src="correlated_bernoulli_reconstruction.png" style="width: 80%;" class="center" />
<figcaption>Fig. 3. VAE with Correlated Relaxed Bernoulli latents. Reconstruction. </figcaption>
<img src="correlated_bernoulli_sample.png" style="width: 65%;" class="center" />
<figcaption>Fig. 4. VAE with Correlated Relaxed Bernoulli latents. Sampling. </figcaption>
<h2 id="conclusion">Conclusion</h2>
<p>In summary, <code>Just Relax It</code> is a powerful tool for researchers and practitioners working with discrete variables in neural networks. By offering a comprehensive set of relaxation techniques, our library aims to make the optimization process more efficient and accessible. We encourage you to explore our library, try out the demo, and contribute to its development. Together, we can push the boundaries of what is possible with discrete variable relaxation in machine learning.</p>
<p>Thank you for reading, and happy coding!</p>
<p><a href="https://github.com/DorinDaniil" target="_blank">Daniil Dorin</a>, <a href="https://github.com/ThunderstormXX" target="_blank">Igor Ignashin</a>, <a href="https://kisnikser.github.io/" target="_blank"><strong>Nikita Kiselev</strong></a>, <a href="https://github.com/Vepricov" target="_blank">Andrey Veprikov</a></p>
<h2 id="references">References</h2>
<p>[1] Kingma and Welling. <a href="https://arxiv.org/abs/1312.6114" target="_blank">&ldquo;Auto-Encoding Variational Bayes&rdquo;</a>. arXiv preprint arXiv:1312.6114 (2013).</p>
<p>[2] Jang et al. <a href="https://arxiv.org/abs/1611.01144" target="_blank">&ldquo;Categorical Reparameterization with Gumbel-Softmax&rdquo;</a>. arXiv preprint arXiv:1611.01144 (2016).</p>
<p>[3] Maddison et al. <a href="https://arxiv.org/abs/1611.00712" target="_blank">&ldquo;The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables&rdquo;</a>. arXiv preprint arXiv:1611.00712 (2016).</p>
<p>[4] Yamada, Lindenbaum et al. <a href="https://arxiv.org/abs/1810.04247" target="_blank">&ldquo;Feature Selection using Stochastic Gates&rdquo;</a>. PMLR, 2020.</p>
<p>[5] Lee, Imrie et al. <a href="https://openreview.net/forum?id=oDFvtxzPOx" target="_blank">&ldquo;Self-Supervision Enhanced Feature Selection with Correlated Gates&rdquo;</a>. ICLR, 2022.</p>
<p>[6] Kool et al. <a href="https://arxiv.org/abs/1903.06059" target="_blank">&ldquo;Stochastic Beams and Where to Find Them: The Gumbel-Top-k Trick for Sampling Sequences Without Replacement&rdquo;</a>. PMLR, 2019.</p>
<p>[7] Bengio et al. <a href="https://arxiv.org/abs/1308.3432" target="_blank">&ldquo;Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation&rdquo;</a>. arXiv preprint arXiv:1308.3432 (2013).</p>
<p>[8] Potapczynski et al. <a href="https://arxiv.org/abs/1912.09588" target="_blank">&ldquo;Invertible Gaussian Reparameterization: Revisiting the Gumbel-Softmax&rdquo;</a>. arXiv preprint arXiv:1912.09588 (2019).</p>
<p>[9] Louizos et al. <a href="https://arxiv.org/abs/1712.01312" target="_blank">&ldquo;Learning Sparse Neural Networks through $L_0$ Regularization&rdquo;</a>. arXiv preprint arXiv:1712.01312 (2017).</p>
<p>[10] Hobbhahn et al. <a href="https://arxiv.org/abs/2003.01227" target="_blank">&ldquo;Fast Predictive Uncertainty for Classification with Bayesian Deep Networks&rdquo;</a>. PMLR, 2022.</p>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/relaxation/">Relaxation</a></li>
      <li><a href="http://localhost:1313/tags/gumbel-softmax/">Gumbel-Softmax</a></li>
      <li><a href="http://localhost:1313/tags/straight-through-estimator/">Straight-Through Estimator</a></li>
      <li><a href="http://localhost:1313/tags/python/">Python</a></li>
      <li><a href="http://localhost:1313/tags/library/">Library</a></li>
      <li><a href="http://localhost:1313/tags/package/">Package</a></li>
      <li><a href="http://localhost:1313/tags/pytorch/">PyTorch</a></li>
      <li><a href="http://localhost:1313/tags/pyro/">Pyro</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="http://localhost:1313/">Nikita Kiselev</a></span> ·     
    <span>
    Powered by 
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/pmichaillat/hugo-website/" rel="noopener" target="_blank">a modified version</a>
         of 
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>
</html>
